[
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Do Phuc Duy Phone Number: 0934821517 Email: doychannel1802@gmail.com University: FPT University Major: Software Engineering Class: Internship Company: Amazon Web Services Vietnam Co., Ltd. Internship Position: FCJ Cloud Intern Internship Duration: From 7/09/2025 to 12/11/2025 My profile picture\nReport Content Worklog Detailed weekly activities and learning progress during the internship period\nProposal Internship project proposals and technical documentation\nTranslated Blogs Technical blog translations and knowledge sharing content\nEvents Participated Workshops, seminars, and training sessions attended\nWorkshop Hands-on technical workshops and practical exercises\nSelf-evaluation Personal assessment and reflection on learning outcomes\nSharing and Feedback Knowledge sharing sessions and feedback from mentors\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/3-ai/3.1/",
	"title": "Create VPC &amp; Configure Security Groups for RDS and Lambda",
	"tags": [],
	"description": "",
	"content": "Create VPC \u0026amp; Configure Security Groups for RDS and Lambda Implementation Steps 1. Access the VPC service Go to AWS Management Console ‚Üí search for VPC.\nSelect Your VPCs ‚Üí Create VPC. Select VPC and more and name the project. In the VPC configuration section: for Number of Availability Zones select 1, for Number of public subnets select 1, for Number of private subnets select 2, and for NAT gateways select Zonal. NAT gateways choose In 1 AZ, VPC endpoints choose None ‚Üí Create VPC.\nThis process will take a few minutes to create the NAT gateway.\nAfter creation is complete, we can view how \u0026lsquo;VPC and more\u0026rsquo; created the resources via the Resource map. We will create an additional private subnet located in a different AZ to be able to create the RDS instance. 2. Set up Security Groups We will create 2 separate Security Groups (SG) for maximum security.\nGo to Security Groups, click Create security group. We will create 2 security groups for Lambda and RDS.\nName the security group and select the VPC we created in step 1, then click create. Continue creating the SG for RDS, select the VPC from step 1. In the inbound rules section, for type select PostgreSQL, and for Source select the SG created for Lambda. "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/4-frontend/4.1/",
	"title": "Hosting website with S3",
	"tags": [],
	"description": "",
	"content": " Step 1: Create an S3 Bucket Go to the S3 service. Click Create bucket. Enter a unique Bucket name (for example: flyora-shop). Uncheck ‚ÄúBlock all public access‚Äù Acknowledge the warning about public access. Click Create bucket. Step 2: Upload Website Files Open your newly created bucket. Click Upload ‚Üí Add files ‚Üí select your website files (e.g, index.html) Click Upload. Step 3: Enable Static Website Hosting Go to the Properties tab of your bucket. Scroll down to Static website hosting. Click Edit ‚Üí Enable Static website hosting Enter: Index document: index.html Error document: index.html (optional) Click Save changes. *Go to the Permission tab of your bucket. Edit Bucket Policy Paste the following JSON policy (replace flyora-shop with your bucket name): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Step 4: Test Your Website Click the Bucket website endpoint URL. http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com\nMake sure it looks like this "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction What is Flyora? Flyora is a modern e-commerce platform designed to demonstrate cloud-native architecture using AWS serverless services. This workshop guides you through building a fully functional online store with product browsing, AI-powered chatbot assistance, and automated data management.\nWorkshop Objectives By completing this workshop, you will:\nDeploy a serverless e-commerce system on AWS Implement automated data pipelines using S3 and Lambda triggers Build RESTful APIs with API Gateway and Lambda Create a scalable database using DynamoDB Integrate AI chatbot functionality for customer support Deploy a static frontend with S3 and CloudFront Set up CI/CD pipelines for automated deployments Architecture Overview The Flyora platform uses a fully serverless architecture:\nFrontend Layer:\nStatic website hosted on Amazon S3 Global content delivery via Amazon CloudFront Responsive UI for product browsing and shopping Backend Layer:\nAPI Gateway for RESTful API endpoints AWS Lambda functions for business logic Amazon DynamoDB for product and order data Amazon S3 for data import and storage AI Layer:\nAI-powered chatbot for product recommendations Integrated into the frontend UI Provides real-time customer assistance Security \u0026amp; Authentication:\nAmazon Cognito for user authentication IAM roles for secure service access Workshop Structure This workshop is organized into team-based modules:\nBackend Team - API development and data pipeline AI Team - Chatbot integration and AI features Frontend Team - UI development and deployment CI/CD - Automated deployment pipeline Testing - System validation and performance testing Cleanup - Resource management and cost optimization Expected Outcomes After completing this workshop, you will have:\nA fully functional e-commerce website running on AWS Hands-on experience with serverless architecture Understanding of AWS best practices for scalability and security Knowledge of CI/CD implementation for cloud applications A portfolio project demonstrating cloud engineering skills Cost Considerations This workshop is designed to run within the AWS Free Tier. All services used have free tier options, and the architecture avoids costly resources like EC2 instances. Estimated cost for running this workshop: $0-5 USD if completed within a few hours.\nPrerequisites Before starting, ensure you have:\nAn AWS account with administrative access Basic understanding of cloud computing concepts Familiarity with REST APIs and JSON Basic knowledge of HTML/CSS/JavaScript (for frontend work) Git installed on your local machine "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/2.1/",
	"title": "Prepare &amp; Configure Lambda Trigger for S3",
	"tags": [],
	"description": "",
	"content": "Prepare \u0026amp; Configure Lambda Trigger for S3 Steps to Perform 1. Access the IAM Service Go to the AWS Management Console ‚Üí search for IAM. Select Roles ‚Üí Create Role. Choose Trusted entity type: AWS service. Choose Use case: Lambda, then click Next. 2. Attach Permissions to the Role Attach the following policies:\nAmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Click Next, then name the role LambdaS3DynamoDBRole.\nThis role allows Lambda to read files from S3 and write data to DynamoDB.\nCreate an S3 Bucket Go to the S3 service. In the S3 interface, select Create bucket. On the Create bucket screen:\nBucket name: Enter a name, for example:\nflyora-bucket-database (If the name already exists, add a number at the end.)\nKeep all other default settings unchanged.\nReview your configuration and click Create bucket to finish. Expected Results The flyora-bucket (or your chosen name) is successfully created. The LambdaS3DynamoDBRole role is ready to be assigned to Lambda in the next step. Configure Lambda Trigger for S3 In this step, you will configure AWS Lambda to automatically import CSV files into DynamoDB whenever a new file is uploaded to the S3 Bucket.\nCreate a Lambda Function Go to Lambda ‚Üí Create function. Select Author from scratch. Function name: AutoImportCSVtoDynamoDB. Runtime: Python 3.13. Role: select LambdaS3DynamoDBRole created in the previous step. Add a Trigger In the Configuration ‚Üí Triggers tab, click Add trigger. Choose S3. Select Bucket flyora-bucket. Event type: All object create events. Click Add to save. Paste the Lambda Code\nPaste the following code: import boto3 import csv import io import json from botocore.exceptions import ClientError from decimal import Decimal dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) # ------------------------- # H√†m ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu c·ªßa m·∫´u (Detect Type) # ------------------------- def detect_type(value): val_str = str(value).strip() # Check Int/Float try: float(val_str) return \u0026#39;N\u0026#39; # Number except ValueError: pass return \u0026#39;S\u0026#39; # String # ------------------------- # H√†m chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu (Convert) # ------------------------- def convert_value(value): if value is None: return None val_str = str(value).strip() if val_str == \u0026#34;\u0026#34;: return None # Int check try: if float(val_str).is_integer(): return int(float(val_str)) except ValueError: pass # Decimal check (cho Float) try: return Decimal(val_str) except Exception: pass # Boolean if val_str.lower() == \u0026#34;true\u0026#34;: return True if val_str.lower() == \u0026#34;false\u0026#34;: return False return val_str # ------------------------- # T·∫°o b·∫£ng Dynamic d·ª±a tr√™n ki·ªÉu d·ªØ li·ªáu ph√°t hi·ªán ƒë∆∞·ª£c # ------------------------- def create_table_if_not_exists(table_name, pk_name, pk_type): existing_tables = dynamodb.meta.client.list_tables()[\u0026#39;TableNames\u0026#39;] if table_name in existing_tables: print(f\u0026#34;Table \u0026#39;{table_name}\u0026#39; already exists.\u0026#34;) return print(f\u0026#34;Creating table: {table_name} | PK: {pk_name} | Type: {pk_type}\u0026#34;) table = dynamodb.create_table( TableName=table_name, KeySchema=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;KeyType\u0026#39;: \u0026#39;HASH\u0026#39;}], AttributeDefinitions=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;AttributeType\u0026#39;: pk_type}], BillingMode=\u0026#39;PAY_PER_REQUEST\u0026#39; ) table.wait_until_exists() print(\u0026#34;Table created successfully.\u0026#34;) # ------------------------- # Main Handler # ------------------------- def lambda_handler(event, context): try: for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] print(f\u0026#34;Processing: {key}\u0026#34;) response = s3.get_object(Bucket=bucket, Key=key) # 1. QUAN TR·ªåNG: D√πng \u0026#39;utf-8-sig\u0026#39; ƒë·ªÉ x√≥a BOM, gi√∫p nh·∫≠n di·ªán s·ªë ch√≠nh x√°c body = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8-sig\u0026#39;) reader = csv.DictReader(io.StringIO(body)) # Clean headers reader.fieldnames = [name.strip() for name in reader.fieldnames] items = list(reader) if not items: continue # L·∫•y th√¥ng tin Partition Key (PK) pk_name = reader.fieldnames[0] table_name = key.split(\u0026#39;.\u0026#39;)[0] # 2. QUAN TR·ªåNG: Ph√°t hi·ªán ki·ªÉu d·ªØ li·ªáu d·ª±a tr√™n d√≤ng ƒë·∫ßu ti√™n first_pk_val = items[0].get(pk_name) pk_type = detect_type(first_pk_val) # S·∫Ω tr·∫£ v·ªÅ \u0026#39;N\u0026#39; n·∫øu l√† s·ªë, \u0026#39;S\u0026#39; n·∫øu l√† ch·ªØ # T·∫°o b·∫£ng ƒë√∫ng ki·ªÉu (N ho·∫∑c S) create_table_if_not_exists(table_name, pk_name, pk_type) table = dynamodb.Table(table_name) count = 0 with table.batch_writer() as batch: for row in items: clean_item = {} is_valid = True for k, v in row.items(): if not k or k.strip() == \u0026#34;\u0026#34;: continue clean_k = k.strip() val = convert_value(v) # Chuy·ªÉn ƒë·ªïi sang Int/Decimal/Bool if val is None: continue # 3. QUAN TR·ªåNG: X·ª≠ l√Ω Partition Key theo ƒë√∫ng ki·ªÉu c·ªßa B·∫£ng if clean_k == pk_name: if pk_type == \u0026#39;N\u0026#39;: # N·∫øu b·∫£ng l√† Number, b·∫Øt bu·ªôc Key ph·∫£i l√† Number if not isinstance(val, (int, Decimal)): print(f\u0026#34;SKIPPING ROW: Key \u0026#39;{val}\u0026#39; is not a number but table requires Number.\u0026#34;) is_valid = False break else: # N·∫øu b·∫£ng l√† String, √©p ki·ªÉu sang String val = str(val) clean_item[clean_k] = val if is_valid and pk_name in clean_item: batch.put_item(Item=clean_item) count += 1 print(f\u0026#34;Success: Imported {count} items into {table_name} (PK Type: {pk_type})\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;OK\u0026#39;)} except Exception as e: print(f\u0026#34;ERROR: {str(e)}\u0026#34;) import traceback traceback.print_exc() return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} Click Deploy and confirm it shows Successfully. "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week1/",
	"title": "Week 1: Foundations",
	"tags": [],
	"description": "",
	"content": "Week 1: AWS Foundations Week 1 Objectives Understand cloud computing fundamentals Set up AWS account and billing Master IAM users, groups, and policies Navigate AWS Management Console Tasks to be carried out: Day Task Start Date Completion Date Reference Material (Link) 1 AWS Account Setup 7/9/2025 7/9/2025 AWS Getting Started Guide 2 Cloud Computing Fundamentals 8/9/2025 8/9/2025 What is Cloud Computing 3 IAM Users and Groups Setup 9/9/2025 9/9/2025 IAM User Guide 4 IAM Policies Configuration 10/9/2025 10/9/2025 IAM Best Practices 5 AWS Console Navigation 11/9/2025 11/9/2025 AWS Management Console 6 AWS Fundamentals Workshop 12/9/2025 12/9/2025 Workshop Materials 7 Week Review and Assessment 13/9/2025 13/9/2025 Practice Scenarios Week 1 Achievements ‚úÖ Successfully created and configured AWS account ‚úÖ Implemented proper IAM security structure ‚úÖ Mastered AWS Management Console navigation ‚úÖ Completed hands-on workshop exercises ‚úÖ Established foundation for advanced AWS learning "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week2/",
	"title": "Week 2: Compute Services",
	"tags": [],
	"description": "",
	"content": "Week 2: AWS Compute Services Week 2 Objectives Launch and manage EC2 instances Configure Auto Scaling groups Create and deploy Lambda functions Understand compute pricing models Tasks to be carried out: Day Task Start Date Completion Date Reference Material (Link) 1 EC2 Instance Types Study 14/9/2025 14/9/2025 EC2 Instance Types 2 Launch First EC2 Instance 15/9/2025 15/9/2025 EC2 User Guide 3 Security Groups Configuration 16/9/2025 16/9/2025 EC2 Security Groups 4 Auto Scaling Setup 17/9/2025 17/9/2025 Auto Scaling User Guide 5 Lambda Function Creation 18/9/2025 18/9/2025 Lambda Developer Guide 6 Hands-on EC2 Lab Session 19/9/2025 19/9/2025 Lab Instructions 7 Compute Services Assessment 20/9/2025 20/9/2025 Practice Scenarios Week 2 Achievements ‚úÖ Successfully launched and configured EC2 instances ‚úÖ Implemented Auto Scaling for high availability ‚úÖ Created and deployed serverless Lambda functions ‚úÖ Mastered compute service pricing optimization ‚úÖ Completed hands-on lab exercises with real scenarios "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week4/",
	"title": "Week 4: Networking &amp; Security",
	"tags": [],
	"description": "",
	"content": "Week 4: Networking \u0026amp; Security Week 4 Objectives Design VPC architectures Configure security groups and NACLs Implement CloudFront distributions Set up Route 53 DNS Tasks to be carried out: Day Task Start Date Completion Date Reference Material (Link) 1 VPC Architecture Design 28/9/2025 28/9/2025 VPC User Guide 2 Subnets and Route Tables Setup 29/9/2025 29/9/2025 VPC Subnets 3 Security Groups and NACLs Configuration 30/9/2025 30/9/2025 Security Groups 4 CloudFront Distribution Setup 1/10/2025 1/10/2025 CloudFront Developer Guide 5 Route 53 DNS Configuration 2/10/2025 2/10/2025 Route 53 Developer Guide 6 Security Best Practices Seminar 3/10/2025 3/10/2025 Seminar Materials 7 Networking and Security Assessment 4/10/2025 4/10/2025 Case Studies Week 4 Achievements ‚úÖ Designed and implemented secure VPC architectures ‚úÖ Configured proper network security with Security Groups and NACLs ‚úÖ Deployed CloudFront CDN for global content delivery ‚úÖ Set up Route 53 DNS with health checks and routing policies ‚úÖ Mastered network security best practices and troubleshooting "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week5/",
	"title": "Week 5: Monitoring &amp; Management",
	"tags": [],
	"description": "",
	"content": "Week 5: Monitoring \u0026amp; Management Week 5 Objectives Set up CloudWatch monitoring Configure CloudTrail logging Use Systems Manager tools Implement automated responses Tasks to be carried out: Day Task Start Date Completion Date Reference Material (Link) 1 CloudWatch Metrics and Alarms Setup 5/10/2025 5/10/2025 CloudWatch User Guide 2 CloudWatch Dashboards Creation 6/10/2025 6/10/2025 CloudWatch Dashboards 3 CloudTrail Configuration 7/10/2025 7/10/2025 CloudTrail User Guide 4 Systems Manager Tools Implementation 8/10/2025 8/10/2025 Systems Manager User Guide 5 CloudFormation Templates 9/10/2025 9/10/2025 CloudFormation User Guide 6 DevOps Tools Deep Dive 10/10/2025 10/10/2025 Session Materials 7 Monitoring and Management Assessment 11/10/2025 11/10/2025 Automation Scenarios Week 5 Achievements ‚úÖ Implemented comprehensive CloudWatch monitoring and alerting ‚úÖ Configured CloudTrail for complete audit logging ‚úÖ Mastered Systems Manager tools for infrastructure management ‚úÖ Created Infrastructure as Code with CloudFormation ‚úÖ Established automated DevOps workflows and cost optimization "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week6/",
	"title": "Week 6: Advanced Topics",
	"tags": [],
	"description": "",
	"content": "Week 6: Advanced Topics Week 6 Objectives Design serverless architectures Deploy containerized applications Implement microservices patterns Present capstone projects Tasks to be carried out: Day Task Start Date Completion Date Reference Material (Link) 1 Serverless Architecture Design 12/10/2025 12/10/2025 Serverless Application Lens 2 API Gateway and Lambda Integration 13/10/2025 13/10/2025 API Gateway Developer Guide 3 Container Services with ECS/EKS 14/10/2025 14/10/2025 ECS Developer Guide 4 Capstone Project Development 15/10/2025 15/10/2025 Project Templates 5 Architecture Review and Testing 16/10/2025 16/10/2025 EKS User Guide 6 Capstone Project Presentations 17/10/2025 17/10/2025 Presentation Guidelines 7 Program Completion and Certification 18/10/2025 18/10/2025 Graduation Materials Week 6 Achievements ‚úÖ Designed and implemented serverless architectures with best practices ‚úÖ Successfully deployed containerized applications using ECS/EKS ‚úÖ Created comprehensive capstone project demonstrating AWS mastery ‚úÖ Presented technical solutions with proper documentation ‚úÖ Completed AWS learning journey with industry-ready skills "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week7/",
	"title": "Week 7",
	"tags": [],
	"description": "",
	"content": "Week 7 (October 19-25, 2025) Revision Period This week was dedicated to revision and consolidation of knowledge gained throughout the internship program.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week8/",
	"title": "Week 8",
	"tags": [],
	"description": "",
	"content": "Week 8 (October 26-30, 2025) Revision Period This week continued the revision process, focusing on final preparation and knowledge consolidation.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week9/",
	"title": "Week 9",
	"tags": [],
	"description": "",
	"content": "Week 9 (November 2-8, 2025) Revision Period This week continued the revision process, focusing on final preparation and knowledge consolidation.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Worklog Weekly activities and learning progress during the internship period.\nWeekly Schedule Week 1: Foundations\nIntroduction to AWS Cloud AWS Account Setup and IAM Basics Week 2: Compute Services\nEC2 Instances and Auto Scaling Lambda Functions Event: AWS Cloud Day Vietnam (September 18, 2025) Week 3: Storage \u0026amp; Databases\nS3 and EBS Storage Solutions RDS and DynamoDB Week 4: Networking \u0026amp; Security\nVPC, Subnets, and Security Groups CloudFront and Route 53 Event: AI-Driven Development Life Cycle (October 3, 2025) Week 5: Monitoring \u0026amp; Management\nCloudWatch and CloudTrail Systems Manager Week 6: Advanced Topics\nServerless Architecture Container Services (ECS/EKS) Event: Data Resiliency in a Cloud-first World (November 15, 2025) Week 7: Revision Period\nKnowledge consolidation and review Week 8: Revision Period\nKnowledge consolidation and review Week 9: Revision Period\nKnowledge consolidation and review "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/",
	"title": "Backend Workshop (API &amp; Data Layer)",
	"tags": [],
	"description": "",
	"content": "Backend: API \u0026amp; Data Pipeline In this workshop, you will:\nUse Amazon S3 to store input data (CSV files). Configure AWS Lambda Trigger to automatically import data into DynamoDB. Create Lambda (API Handler) and expose it through API Gateway to access DynamoDB data. Test REST API endpoints via Postman or API Gateway Console. Clean up all resources after completion. "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/3-ai/3.2/",
	"title": "Configure RDS and Connect with DBeaver",
	"tags": [],
	"description": "",
	"content": "Configure RDS and Connect with DBeaver Implementation Steps 1. Access RDS Service Go to AWS Management Console ‚Üí search for RDS. Before creating the RDS instance, we will create a subnet group. Name the subnet group and select the VPC you created. For AZ select ap-southeast-1a and ap-southeast-1b. For Subnets, select the 2 private subnets, then click Create. 2. Create RDS Go to databases ‚Üí Create database. In the database configuration section, select Full configuration, select PostgreSQL. In Templates select Sandbox. In Settings, name the DB and set a password. Configure the remaining parts as follows: In the Connectivity section, select the VPC created and the DB subnet group. Set Public access to No. In the VPC security group section, select the Security group created for RDS. Keep the rest as default and click Create. 1. Store Data and Connect to PostgreSQL using DBeaver You can download DBeaver here: https://dbeaver.io/ Download the knowledge_base file from here.\nTo connect from the local machine to DBeaver, we need to create an EC2 instance to act as a bridge.\nAccess EC2 ‚Üí Launch instance.\nName the EC2, select Instance type t2.micro, create a key pair, and save it to your machine. In Network settings, select the VPC created, select a public subnet, and create a Security group. In Inbound Security Group Rules, select \u0026lsquo;My IP\u0026rsquo;, then Launch instance. Next, go to the RDS Security group section, edit the inbound rules, and add a new rule. Type: PostgreSQL and Source: the newly created EC2. Open DBeaver and click on the connection section. Select PostgreSQL. In the Host section, copy the RDS Endpoint. Fill in the information you used when creating the RDS. Click on the SSH tab. In Host/IP, copy the Public IP of the EC2. For User Name, enter ec2-user. In Authentication Method, select Public Key and choose the key pair created with the EC2. Then click Test connection and Finish. After successfully connecting, open an SQL script and paste this code to create the knowledge_base table. After creating the table, refresh the database to show the knowledge_base table.\n-- 1. Enable vector extension (run only once) CREATE EXTENSION IF NOT EXISTS vector; -- 2. Create knowledge base table (Knowledge Base) CREATE TABLE knowledge_base ( id bigserial PRIMARY KEY, content text, -- Original text content (chunked text) metadata jsonb, -- Store extra info: image link, filename, page number... embedding vector(1024) -- IMPORTANT: Must be 1024 for Cohere Multilingual ); -- 3. Create index for faster search CREATE INDEX ON knowledge_base USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64); To import data into DBeaver using Python, we need to SSH via CMD. Open CMD in the folder containing the keypair and copy this command:\nssh -i \u0026#34;my-key.pem\u0026#34; -L 5433:RDS endpoint port:5432 ec2-user@public IP EC2 -N Then, run this Python script to import data into DBeaver.\nimport pandas as pd import json import boto3 import psycopg2 import time import glob import os import numpy as np from dotenv import load_dotenv # ========================================== # 1. CONFIGURATION \u0026amp; SECURITY # ========================================== current_dir = os.path.dirname(os.path.abspath(__file__)) env_path = os.path.join(current_dir, \u0026#39;pass.env\u0026#39;) load_dotenv(env_path) CSV_FOLDER = \u0026#39;./database\u0026#39; DB_HOST = os.getenv(\u0026#34;DB_HOST\u0026#34;) DB_NAME = os.getenv(\u0026#34;DB_NAME\u0026#34;) DB_USER = os.getenv(\u0026#34;DB_USER\u0026#34;) DB_PASS = os.getenv(\u0026#34;DB_PASS\u0026#34;) # AWS Connection bedrock = boto3.client( service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;, aws_access_key_id=os.getenv(\u0026#34;aws_access_key_id\u0026#34;), aws_secret_access_key=os.getenv(\u0026#34;aws_secret_access_key\u0026#34;) ) # ========================================== # 2. TRANSLATION DICTIONARY (MOST IMPORTANT) # ========================================== # A. Translate Column Names (For AI context) COLUMN_MAP = { \u0026#34;price\u0026#34;: \u0026#34;Price\u0026#34;, \u0026#34;gia\u0026#34;: \u0026#34;Price\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;Cost\u0026#34;, \u0026#34;fee\u0026#34;: \u0026#34;Fee\u0026#34;, \u0026#34;stock\u0026#34;: \u0026#34;Stock\u0026#34;, \u0026#34;so_luong\u0026#34;: \u0026#34;Stock\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Description\u0026#34;, \u0026#34;mo_ta\u0026#34;: \u0026#34;Description\u0026#34;, \u0026#34;chi_tiet\u0026#34;: \u0026#34;Details\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;Origin\u0026#34;, \u0026#34;xuat_xu\u0026#34;: \u0026#34;Origin\u0026#34;, \u0026#34;material\u0026#34;: \u0026#34;Material\u0026#34;, \u0026#34;chat_lieu\u0026#34;: \u0026#34;Material\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;mau_sac\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;Weight\u0026#34;, \u0026#34;trong_luong\u0026#34;: \u0026#34;Weight\u0026#34;, \u0026#34;food_type\u0026#34;: \u0026#34;Food Type\u0026#34;, \u0026#34;usage_target\u0026#34;: \u0026#34;Suitable for bird species\u0026#34;, \u0026#34;furniture_type\u0026#34;: \u0026#34;Furniture Type\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;Processing Time\u0026#34;, \u0026#34;thoi_gian\u0026#34;: \u0026#34;Processing Time\u0026#34;, \u0026#34;method_name\u0026#34;: \u0026#34;Method Name\u0026#34; } # B. Translate Values (For Website display) \u0026lt;--- PART YOU NEED VALUE_TRANSLATIONS = { \u0026#34;FOODS\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;Foods\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;foods\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;TOYS\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;Toys\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;toys\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;FURNITURE\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;Furniture\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;furniture\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;Bird\u0026#34;: \u0026#34;Pet Bird\u0026#34;, \u0026#34;bird\u0026#34;: \u0026#34;Pet Bird\u0026#34; } # --- AUXILIARY FUNCTIONS --- def get_embedding(text): try: if not text or len(str(text)) \u0026lt; 5: return None body = json.dumps({\u0026#34;texts\u0026#34;: [str(text)], \u0026#34;input_type\u0026#34;: \u0026#34;search_document\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34;}) response = bedrock.invoke_model(body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34;) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except: return None def clean(val): if pd.isna(val) or str(val).lower() in [\u0026#39;nan\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;null\u0026#39;]: return \u0026#34;\u0026#34; val_str = str(val).strip() # --- AUTO TRANSLATE HERE --- # If the value exists in the translation dictionary, replace it immediately if val_str in VALUE_TRANSLATIONS: return VALUE_TRANSLATIONS[val_str] return val_str def main(): try: conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS, port=5433) # Note the SSH port 5433 cur = conn.cursor() print(\u0026#34;‚úÖ Database connection successful!\u0026#34;) except Exception as e: print(f\u0026#34;‚ùå DB Connection Error: {e}\u0026#34;); return csv_files = glob.glob(os.path.join(CSV_FOLDER, \u0026#34;*.csv\u0026#34;)) print(f\u0026#34;üìÇ Found {len(csv_files)} CSV files.\u0026#34;) # Statistics variables stats = {\u0026#34;bird\u0026#34;: 0, \u0026#34;food\u0026#34;: 0, \u0026#34;toy\u0026#34;: 0, \u0026#34;furniture\u0026#34;: 0, \u0026#34;best_sellers\u0026#34;: []} total_success = 0 for file_path in csv_files: filename = os.path.basename(file_path).lower() print(f\u0026#34;\\n--- Processing file: {filename} ---\u0026#34;) try: df = pd.read_csv(file_path) df = df.replace({np.nan: None}) # Auto-detect category (Prefix) category_prefix = \u0026#34;Product\u0026#34; if \u0026#34;bird\u0026#34; in filename: category_prefix = \u0026#34;Bird Species\u0026#34; elif \u0026#34;food\u0026#34; in filename: category_prefix = \u0026#34;Bird Food\u0026#34; elif \u0026#34;toy\u0026#34; in filename or \u0026#34;do_choi\u0026#34; in filename: category_prefix = \u0026#34;Bird Toy\u0026#34; elif \u0026#34;furniture\u0026#34; in filename: category_prefix = \u0026#34;Cage Furniture\u0026#34; elif \u0026#34;ship\u0026#34; in filename or \u0026#34;delivery\u0026#34; in filename: category_prefix = \u0026#34;Shipping Method\u0026#34; elif \u0026#34;payment\u0026#34; in filename: category_prefix = \u0026#34;Payment Method\u0026#34; for index, row in df.iterrows(): # Statistics if \u0026#34;bird\u0026#34; in filename: stats[\u0026#34;bird\u0026#34;] += 1 elif \u0026#34;food\u0026#34; in filename: stats[\u0026#34;food\u0026#34;] += 1 elif \u0026#34;toy\u0026#34; in filename: stats[\u0026#34;toy\u0026#34;] += 1 elif \u0026#34;furniture\u0026#34; in filename: stats[\u0026#34;furniture\u0026#34;] += 1 # A. IDENTITY p_id = clean(row.get(\u0026#39;id\u0026#39;) or row.get(\u0026#39;product_id\u0026#39;) or row.get(\u0026#39;payment_id\u0026#39;)) name = clean(row.get(\u0026#39;name\u0026#39;) or row.get(\u0026#39;product_name\u0026#39;) or row.get(\u0026#39;title\u0026#39;) or row.get(\u0026#39;method_name\u0026#39;)) if not name: if p_id: name = f\u0026#34;Code {p_id}\u0026#34; else: continue # B. AUTO SCAN COLUMNS AND TRANSLATE content_parts = [f\u0026#34;{category_prefix}: {name}\u0026#34;] # Scan all columns, if exists in COLUMN_MAP then add for col_key, col_val in row.items(): val_clean = clean(col_val) # clean function will auto translate FOODS -\u0026gt; Food if val_clean and col_key in COLUMN_MAP: content_parts.append(f\u0026#34;{COLUMN_MAP[col_key]}: {val_clean}\u0026#34;) # C. HANDLE PRICE \u0026amp; STOCK \u0026amp; BEST SELLERS SEPARATELY price = clean(row.get(\u0026#39;price\u0026#39;) or row.get(\u0026#39;gia\u0026#39;) or row.get(\u0026#39;fee\u0026#39;)) if price: content_parts.append(f\u0026#34;Price: {price}\u0026#34;) stock = clean(row.get(\u0026#39;stock\u0026#39;) or row.get(\u0026#39;so_luong\u0026#39;)) if stock: content_parts.append(f\u0026#34;Stock: {stock}\u0026#34;) sold = clean(row.get(\u0026#39;sold\u0026#39;) or row.get(\u0026#39;da_ban\u0026#39;)) if sold: content_parts.append(f\u0026#34;Sold: {sold}\u0026#34;) try: if float(sold) \u0026gt; 0: stats[\u0026#34;best_sellers\u0026#34;].append((float(sold), name, category_prefix)) except: pass content_to_embed = \u0026#34;. \u0026#34;.join(content_parts) + \u0026#34;.\u0026#34; # D. CREATE METADATA (Also use translated values) # Note: The clean() function above already translated, so we call clean() again for each field metadata = {} for k, v in row.items(): metadata[k] = clean(v) # Save to metadata in English # Overwrite standard fields metadata[\u0026#39;id\u0026#39;] = p_id metadata[\u0026#39;name\u0026#39;] = name metadata[\u0026#39;price\u0026#39;] = price if price else \u0026#34;Contact\u0026#34; metadata[\u0026#39;type\u0026#39;] = category_prefix metadata[\u0026#39;image\u0026#39;] = clean(row.get(\u0026#39;image_url\u0026#39;) or row.get(\u0026#39;link_anh\u0026#39;)) metadata[\u0026#39;sold\u0026#39;] = sold # E. INSERT vector = get_embedding(content_to_embed) if vector: cur.execute( \u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (content_to_embed, json.dumps(vector), json.dumps(metadata, default=str)) ) total_success += 1 if total_success % 10 == 0: print(f\u0026#34; -\u0026gt; Loaded {total_success} rows...\u0026#34;) conn.commit() time.sleep(0.1) except Exception as e: print(f\u0026#34;‚ö†Ô∏è Error processing file {filename}: {e}\u0026#34;); continue # --- CREATE STATISTICS REPORT --- print(\u0026#34;\\n--- Creating statistics report... ---\u0026#34;) top_products = sorted(stats[\u0026#34;best_sellers\u0026#34;], key=lambda x: x[0], reverse=True)[:5] top_names = \u0026#34;, \u0026#34;.join([f\u0026#34;{p[1]} ({int(p[0])} purchases)\u0026#34; for p in top_products]) summary_content = ( f\u0026#34;BIRD SHOP STATISTICS REPORT: \u0026#34; f\u0026#34;Total birds: {stats[\u0026#39;bird\u0026#39;]}. Food: {stats[\u0026#39;food\u0026#39;]}. \u0026#34; f\u0026#34;Toys: {stats[\u0026#39;toy\u0026#39;]}. Furniture: {stats[\u0026#39;furniture\u0026#39;]}. \u0026#34; f\u0026#34;TOP 5 BEST SELLING PRODUCTS: {top_names}.\u0026#34; ) summary_vector = get_embedding(summary_content) if summary_vector: cur.execute(\u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (summary_content, json.dumps(summary_vector), json.dumps({\u0026#34;id\u0026#34;:\u0026#34;STATS\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Statistics\u0026#34;}, default=str))) conn.commit() cur.close(); conn.close() print(f\u0026#34;\\nüéâ COMPLETED! Total imported: {total_success + 1} rows.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() After importing, refresh the knowledge_base table to see the results.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/4-frontend/4.2/",
	"title": "Distribute with CloudFront ",
	"tags": [],
	"description": "",
	"content": " Create a CloudFront Distribution pointing to the S3 bucket Go to the Cloudfront service. Create Cloudfront Distribution Enter a Cloudfront name (for example: flyora-shop). Select your S3 you have created before Turn on Website endpoint make sure you get a url like this http://Your-S3-name.s3-website.ap-southeast-1.amazonaws.com/ Configure: Viewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP method: GET, HEAD , OPTION Cache policy: CatchingOptimized Response headers policy: CORS-with-preflight-and-SecurityHeadersPolicy Choose do not enable securiy protections Review and click Create Distribution After Create Distribution you have to wait for 5 or 10 minutes to deploy and if it deploy successfully it will show you date you have deploy "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/2.2/",
	"title": "Verify Data in DynamoDB",
	"tags": [],
	"description": "",
	"content": "Verify Data in DynamoDB In this step, you will verify that the data from the CSV file has been successfully imported into DynamoDB.\nSteps to Perform Upload the CSV File to the Bucket Download the sample CSV file from here.\nIn the newly created Bucket:\nGo to the Objects tab ‚Üí click Upload. Extract file zip. Drag and drop the files, then click Upload. [!TIP] After uploading, please wait about 3‚Äì5 minutes for the Lambda function to import the data.\nAccess the DynamoDB Service Go to AWS Management Console ‚Üí search for DynamoDB. Select Tables ‚Üí click on the products table. View the Data Open the Explore items tab. Check the list of products that have been imported. If you don\u0026rsquo;t see any data, please check the following:\nThe DynamoDB table name must match the CSV file name. The CSV file must have a valid header. The Lambda function must have sufficient permissions to access both S3 and DynamoDB. "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/3.-translated-blogs/blog3/",
	"title": " AWS collaborates with Chugai Pharmaceutical Co. on quantum-inspired and constraint programming methods for cyclic peptide-protein docking",
	"tags": [],
	"description": "",
	"content": "Quantum Technology Blog AWS AWS collaborates with Chugai Pharmaceutical Co. on quantum-inspired and constraint programming methods for cyclic peptide-protein docking\nAuthors: Kyle Brubaker, Akihiko Arakawa, Fabian Furrer, Jayeeta Ghosh, Helmut Katzgraber, and Kyle Booth ‚Äî September 24, 2025 at Amazon Quantum Solutions Lab, Quantum Technologies\nOriginal link: https://aws.amazon.com/blogs/quantum-computing/aws-collaborates-with-chugai-pharmaceutical-co-on-quantum-inspired-and-constraint-programming-methods-for-cyclic-peptide-protein-docking/\nWhen we have a three-dimensional model of a potential drug target (binding site), what is the orientation of the candidate drug molecule that \u0026ldquo;fits\u0026rdquo; (docks) with that binding site? Structure-based drug design involves using computational tools primarily to answer this question!\nChugai Pharmaceutical Co., Ltd., a leading Japanese pharmaceutical company, specializes in research, development, manufacturing, and marketing of prescription drugs in the fields of oncology, immunology/allergy, neurology, and kidney disease. In an effort to advance molecular research through quantum computing technology, Chugai has partnered with Amazon Web Services (AWS) to explore the application of quantum-compatible computational techniques to cyclic peptide simulation, aiming to accelerate drug discovery.\nPeptides are short chains of amino acids linked together by peptide bonds, typically containing about 2‚Äì50 amino acid residues. Cyclic peptides are peptides with a closed ring structure. Although cyclic peptide drugs have many advantages over traditional small molecules [1, 2], simulating their docking with exogenous proteins is time-consuming and error-prone due to the larger number of rotational bonds.\nIn this article, we summarize AWS\u0026rsquo;s efforts in collaboration with Chugai to construct the cyclic peptide docking problem as a Quadratic Unconstrained Binary Optimization (QUBO) model, which is suitable for quantum computing. Modeling the problem as QUBO allows the application of both classical and quantum optimization methods, such as classical and quantum annealing, to optimize peptide particle positions. We also developed a Constraint Programming (CP) model used as a baseline method. These methods were tested on example cases from the Protein Data Bank (PDB), and we used an end-to-end framework to predict the three-dimensional structure of multiple cyclic peptides in environments with exogenous proteins. Both methods successfully predicted configurations, however, the QUBO-based method encountered scalability limitations when exceeding six peptides and 34 protein residues, while the CP method could solve all cases in the test set.\nFor more details, please refer to our scientific paper published in Scientific Reports.\nRepresentation of cyclic peptides and exogenous proteins\nTo represent peptides, we use a \u0026ldquo;coarse-grained\u0026rdquo; representation for each amino acid, where main atoms and side chains are represented by single particles. We use two particles for all amino acids except Glycine, which has no side chain and therefore requires only a single particle (Figure 1).\nFigure 1 ‚Äì Example of coarse-grained representation of Glycine (G) with 1 particle and Lysine (K) with 2 particles. Blue is used for the main chain, and green for the side chain.\nWe place these peptide particles on a three-dimensional tetrahedral lattice, as illustrated in Figure 2. Interactions between amino residues are calculated using the Miyazawa-Jernigan (MJ) potential [3]. The goal is to find the optimal arrangement of particles on the lattice such that the total interaction energy is minimized, while ensuring cyclization conditions and avoiding particle overlap.\nFigure 2 ‚Äì Feasible arrangement of cyclic peptide P-K-I-D-N-G on a tetrahedral lattice, with blue representing the main chain and green representing side chains. The cyclization bond is colored red. Some free vertices of the tetrahedral lattice are illustrated in white.\nWe only consider the \u0026ldquo;active site\u0026rdquo; of the target protein ‚Äì a concentrated region where the peptide is likely to bind. This helps reduce the problem space and increase the likelihood of finding the optimal peptide orientation. To identify the active site in the target protein, we identify amino residues within a specified distance (in this case 5√Ö) from the peptide\u0026rsquo;s amino residues. The protein affects the peptide on the lattice in two ways: it blocks some positions that are too close (within 3.8√Ö) where the peptide cannot fit, and it creates interaction regions (within 6.5√Ö) where protein and peptide can interact. After removing blocked positions, we calculate the energy between each part of the peptide and nearby protein parts at interaction points. The distances chosen for the active site, blocked region, and interaction region will affect the solution the system finds.\nQUBO Model\nWe investigate modeling and solving this problem using Quadratic Unconstrained Binary Optimization (QUBO), which is suitable for quantum properties. QUBO is a combinatorial optimization problem aimed at minimizing a quadratic function with binary variables (0 or 1). In this work, we start by representing the problem as high-order binary optimization, then use locality reduction techniques to convert to QUBO form.\nMany Hamiltonian expressions for non-docking protein folding problems have been introduced and analyzed with different encoding strategies. The original work by Perdomo et al. [4] used spatial encoding to map protein positions to a 2D lattice. Later, researchers developed turn encoding methods [5], describing protein shape through sequences of steps, making it more efficient for long proteins. Many groups have improved this method: Babbush et al. [5] created the \u0026ldquo;turn ancilla\u0026rdquo; method, while other groups adapted it for different types of spatial lattices and quantum computing methods.\nOur model is an extension of the \u0026ldquo;resource-efficient\u0026rdquo; turn encoding method [6]. The original model includes interactions between peptides, non-overlap constraints between non-consecutive particles, and no-return constraints to prevent consecutive particles from overlapping. Therefore, the Hamiltonian expression consists of two parts: Hcomb, handling interactions between peptides and non-overlap errors; and Hback, ensuring consecutive particles do not return to the same lattice point. To extend this model to integrate external proteins in cyclic peptides, we add two components: Hcycle to impose peptide cyclization conditions, and Hprotein to handle peptide‚Äìprotein interactions and steric collisions.\nThe Hcycle component ensures cyclization between specific amino acid pairs by requiring them to be nearest neighbors (1-NN) on the lattice, using the distance function d(x, y) to calculate the number of lattice steps between vertices. For peptide‚Äìprotein interactions, Hprotein includes MJ interaction energy for peptide residues within the protein\u0026rsquo;s interaction region and penalties for blocked positions due to steric collisions.\nThe final expression combines all components, extending the original model to include cyclization factors and protein interactions:\nH = Hcomb + Hback + Hcycle + Hprotein\nHcomb : Combines interaction energy between peptides with overlap penalties\nHback : Imposes no-return constraints for consecutive particles\nHcycle : Enforces cyclization constraints\nHprotein : Handles peptide‚Äìprotein interactions and steric collisions\nConstraint Programming (CP) Model\nAs a baseline, we built a Constraint Programming (CP) model for the problem. CP is a method for solving constraint satisfaction and optimization problems, including both problem modeling and backtracking-based solvers. Typically, users describe the problem with specific variables and constraints, then the search process tries different possibilities to find solutions. The term \u0026ldquo;programming\u0026rdquo; here means \u0026ldquo;planning\u0026rdquo; rather than \u0026ldquo;coding.\u0026rdquo;\nMain constraints in the CP model:\nPlace amino residues at valid lattice points, excluding positions affected by steric collisions from external proteins\nMaintain precise distances between bonded amino residues\nPrevent overlap between amino residues\nImpose cyclization bonds between amino residues\nThe model\u0026rsquo;s objective function is to minimize the total MJ interaction energy between adjacent particles. The CP model is designed to optimize interaction energy between amino residues on the lattice while complying with various spatial constraints. CP can flexibly handle logical constraints and nonlinear relationships that linear programming models find difficult to implement. The model ensures amino residues are placed correctly on the lattice, avoiding overlap, and considering interactions within allowable ranges.\nResults\nTo evaluate the methods, we built a comprehensive processing pipeline for protein‚Äìpeptide complexes from the Protein Data Bank (PDB) to predict the optimal three-dimensional structure of bound peptides. The PDB repository contains detailed and standardized atomic coordinates for ring protein complexes. The pipeline consists of the following steps, as illustrated in Figure 3:\nSeparate the PDB file into two parts: peptide and target protein\nIdentify the protein\u0026rsquo;s active site ‚Äì the concentrated region where the peptide is likely to bind\nClassify each atom in the peptide as main chain or side chain, then calculate the weighted center of mass of each group, creating a 2-particle representation for each amino acid\nCreate a tetrahedral lattice to discretize the search space for discrete optimization methods\nFilter the lattice based on active site coordinates and steric collision radius to remove collision-causing vertices, creating the final lattice structure\nFigure 3 ‚Äì Conversion from atomic representation to coarse-grained 2-particle representation on tetrahedral lattice\nPDB serves as standard data for comparison with model predictions. However, the tetrahedral lattice has some limitations: actual peptides are not fixed to rigid structures, but their shapes are determined by natural forces. Therefore, comparing predicted structures with actual structures from PDB is not straightforward. Nevertheless, we evaluate the model\u0026rsquo;s solutions based on two criteria: MJ interaction energy value and root mean square distance (RMSD) compared to actual peptide coordinates (from PDB files) in Euclidean space. RMSD measures the average distance between coarse particles of actual peptides and model-generated peptides. Any RMSD value exceeding 4√Ö is considered insufficiently accurate for practical use.\nPDB Code Peptide Sequence Number of amino residues Peptide External Protein 3WNE P-K-I-D-N-G 6 26 5LSO K-S-R-W-D-E 6 34 3AV9 S-A-K-I-D-N-L-D 8 28 3AVI S-L-K-I-D-N-M-D 8 32 3AVN S-H-K-I-D-N-L-D 8 28 2F58 H-I-G-P-G-R-A-F-G-G-G 11 49 Table 1: Peptide‚Äìprotein complexes from the RCSB Protein Databank (https://www.rcsb.org/) used for our experimental analysis.\nWe analyzed six peptide‚Äìprotein complexes from PDB, with peptide amino residue counts ranging from 6 to 11 and protein active site amino residue counts ranging from 26 to 49, as shown in Table 1. For the QUBO model, we applied simulated annealing ‚Äì a heuristic algorithm, implemented in an open-source Python hybrid framework. Hyperparameter optimization was performed using Bayesian optimization through Amazon SageMaker. The CP model was implemented using the OR-Tools library and solved optimally within a 300-second time limit on Amazon EC2 m5.4xlarge virtual machines.\nPDB ID Without External Protein With External Protein MJ potential RMSD (√Ö) MJ potential RMSD (√Ö) QUBO CP QUBO CP QUBO CP QUBO CP 3WNE -1.87 -1.87 10.03 7.77 -28.53 -42.64 8.88 8.48 5LSO -5.93 -5.97 13.31 13.20 -17.93 -23.26 7.68 10.29 3AV9 -6.18 -6.64 11.36 9.92 ‚Äì -40.44 ‚Äì 8.98 3AVI -7.43 -8.34 9.65 9.39 ‚Äì -62.34 ‚Äì 8.74 3AVN -6.52 -6.88 10.60 10.75 ‚Äì -55.12 ‚Äì 8.34 2F58 -6.18 -13.61 11.52 11.39 ‚Äì -4.02 ‚Äì 10.42 Table 2: Comparison of MJ interaction energy values and RMSD for QUBO and CP methods, with and without external protein.\nTable 2 compares performance metrics (MJ interaction energy and RMSD) between the two QUBO and CP methods for each peptide, with and without external protein, as there was no benchmark from previous studies to compare our results.\nThe QUBO method found feasible solutions for problems with peptides containing up to 6 amino residues and target proteins with 34 amino residues. A feasible solution is a configuration that satisfies all problem constraints but may not be optimal in terms of MJ interaction energy. However, for larger cases, QUBO often struggled and produced solutions that violated constraints, because in the QUBO model constraints must be incorporated into the objective function as penalty functions. When there was no external protein, QUBO successfully solved all test cases, achieving the lowest energy scores in 5/6 cases, except for the 2F58 complex which performed worse than CP. Both methods achieved equivalent accuracy (by RMSD) in matching actual structures. However, when protein was present, QUBO only succeeded with 2/5 peptides (3WNE and 5LSO), and achieved higher energy scores than CP in both cases. Notably, QUBO showed higher accuracy than CP for the 5LSO peptide based on RMSD.\nFigure 4 illustrates how both QUBO and CP methods successfully modeled the 5LSO peptide in the presence of protein, producing valid structures compared to the actual structure in the protein database. This shows that optimizing energy scores does not always lead to more accurate structure prediction, especially when starting from non-optimal positions on the tetrahedral lattice model.\nThe CP method showed scalability, solving problems with up to 11 peptide amino residues and 49 target protein amino residues optimally.\nFigure 4: Results image of 5LSO peptide. Comparison of actual peptide in PDB (left) with QUBO model results (center) and CP-MJ 1-NN results (right). Peptides are represented by blue and green dots (corresponding to main chain and side chain), while red dots represent amino residues of the external protein.\nConclusion\nIn this article, we introduced a binary model suitable for quantum properties for the cyclic peptide binding problem. Two QUBO and CP methods were compared on a set of peptide cases, where QUBO found feasible solutions for small problems (up to 6 peptide amino residues and 34 protein amino residues) with competitive MJ interaction energy, while CP showed better scalability by optimally solving larger problems (up to 11 peptide amino residues and 49 protein amino residues), as illustrated in the 5LSO peptide case.\nThe QUBO method worked effectively in protein-free environments, nearly equivalent to CP in solution quality. However, QUBO struggled to ensure feasibility when protein was present, along with inherent challenges in representing distance-dependent interactions in the model, suggesting that QUBO-based quantum optimization techniques may not be the most suitable approach for this specific application. Despite limitations, especially regarding scalability, the QUBO method remains a potential foundation for future research as quantum algorithms and hardware continue to develop.\nThe performance of the CP method also demonstrates the long-term suitability of traditional optimization methods in computational biology. As this field advances, hybrid methods combining quantum and classical techniques may be the most effective choice for solving complex biological molecular modeling problems. Looking ahead, the robust performance and scalability of the CP method make it a promising development direction in peptide‚Äìprotein interaction research. These findings not only contribute to understanding computational protein modeling but also provide useful guidance for researchers in choosing appropriate methods for similar biological optimization problems.\nThe use of lattice structures to model cyclic peptide binding, while helping to simplify the problem, brings inherent limitations because actual peptides are not constrained to rigid structures but follow natural steric forces. This simplification contributes to significant RMSD (root mean square deviation) values when comparing model results with actual structures from the Protein Data Bank (PDB).\nFor those interested in peptide drug design or computational structural biology, this research provides valuable insights into current algorithmic methods and their associated trade-offs. It also emphasizes the importance of continued research to bridge the gap between simple computational models and the complex reality of biological molecular interactions. You can read our paper in Scientific Reports to learn more about the methodology, implementation, and overall process. If you need support with similar challenges, please contact AWS to learn how AWS Professional Services and Amazon Quantum Solutions Lab can assist you.\nReferences\n[1] L. Wang, N. Wang, W. Zhang, X. Cheng, Z. Yan, G. Shao, X. Wang, R. Wang, C. Fu, \u0026ldquo;Therapeutic peptides: current applications and future directions\u0026rdquo;. In Signal Transduction and Targeted Therapy, volume 7 (number 1), 2022.\n[2] A. Ohta et al., \u0026ldquo;Validation of new methods for creating oral drugs that overcome the Rule of 5 for difficult-to-access intracellular targets\u0026rdquo;. Journal of the American Chemical Society, volume 145 (number 44), 2023.\n[3] S. Miyazawa, R. Jernigan, \u0026ldquo;Estimation of effective contact energy between amino residues from protein crystal structures: quasi-chemical approximation\u0026rdquo;, Macromolecules, volume 18 (number 3), 1985.\n[4] A. Perdomo, C. Truncik, I. Tubert-Brohman, G. Rose, A. Aspuru-Guzik, \u0026ldquo;Construction of model Hamiltonians for adiabatic quantum computation and its application to finding low-energy conformations of lattice protein models\u0026rdquo;, Physical Review A, volume 78, article 012320.\n[5] R. Babbush, A. Perdomo-Ortiz, B. O\u0026rsquo;Gorman, W. Macready, A. Aspuru-Guzik, \u0026ldquo;Construction of energy functions for lattice heteropolymer models: efficient encoding for constraint satisfaction programming and quantum annealing\u0026rdquo;, Advances in Chemical Physics, John Wiley \u0026amp; Sons, Inc., 2014, pages 201‚Äì244.\n[6] A. Robert, P. K. Barkoutsos, S. Woerner, I. Tavernelli, \u0026ldquo;Resource-efficient quantum algorithm for protein folding\u0026rdquo;, npj Quantum Information, volume 7, article 38.\nTAGS: Amazon Quantum Solutions Lab, quantum algorithms, quantum computing, Quantum Technologies\nKyle Brubaker\nKyle Brubaker is a Senior Applied Scientist at AWS\u0026rsquo;s Advanced Solutions Lab. He received his Master\u0026rsquo;s degree in Biomedical Engineering from New York University (NYU), specializing in brain‚Äìcomputer interfaces. Kyle has an industrial background in machine learning and machine learning engineering, and has recently transitioned to research in quantum computing, optimization, and hybrid machine learning solutions.\nAkihiko Arakawa\nAkihiko Arakawa is a data scientist at Chugai Pharmaceutical Company. He specializes in computational chemistry in small and medium molecule drug development research. Additionally, he leads a research group applying quantum computing to drug development processes. Akihiko received his PhD in structural biology from the University of Tokyo.\nFabian Furrer\nFabian Furrer is a Senior Researcher at AWS\u0026rsquo;s Advanced Solutions Lab. Fabian is passionate about helping customers optimize business operations using classical and quantum optimization techniques. He holds a PhD in quantum information theory from Leibniz University Hanover, and was a postdoctoral researcher at the University of Tokyo and NTT Basic Research Laboratory in Japan. Fabian also has many years of industry experience as a quantitative risk manager in the energy economics field at a utility company.\nJayeeta Ghosh\nAs a Senior Data Scientist at AWS Professional Services, Jayeeta Ghosh collaborates with customers to solve complex business challenges across multiple engineering and industrial fields. She holds PhD and Master\u0026rsquo;s degrees in Chemical Engineering from UC Davis, then conducted postdoctoral research at Rutgers University and the University of Illinois, Urbana-Champaign. Before transitioning to data science consulting roles at Trace3 and QuaEra, she was a Senior Scientist at Simulations Plus, where she developed machine learning and artificial neural network (ML/ANN) models for drug candidate screening using Cheminformatics.\nHelmut Katzgraber\nDr. Helmut Katzgraber received his bachelor\u0026rsquo;s degree in physics from ETH Zurich, and Master\u0026rsquo;s and PhD degrees in physics from the University of California Santa Cruz. After postdoctoral research positions at the University of California Davis and ETH Zurich, he was awarded a professorship fellowship from the Swiss National Science Foundation. In 2009, he joined Texas A\u0026amp;M University as an assistant professor and became a full professor in 2015. Katzgraber joined Microsoft in 2018 as a senior research manager, before moving to Amazon in 2020. He was elected Fellow of the American Physical Society in 2021 and currently leads the Advanced Solutions Lab at AWS.\nKyle Booth\nKyle Booth is a Senior Applied Scientist at AWS\u0026rsquo;s Advanced Solutions Lab. He received his PhD in Operations Research from the University of Toronto. His research focuses on constraint programming and integer programming to solve combinatorial optimization problems. Before joining AWS, he was a scientist at NASA Ames Research Center.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/3.-translated-blogs/blog2/",
	"title": " Humane World for Animals uses AWS to scale global animal welfare programs",
	"tags": [],
	"description": "",
	"content": " Humane World for Animals AWS Implementation\nHumane World for Animals uses AWS to scale global animal welfare programs by Stacy Stonich and Jules Marenghi | on 24 SEP 2025 | in Customer Solutions, Nonprofit, Public Sector | Permalink\nFor over 70 years, Humane World for Animals‚Äîformerly the Humane Society of the United States and Humane Society International‚Äîhas been a pioneer in the animal protection movement, addressing the root causes of animal cruelty to drive lasting change and create a better world for all animals globally. The organization has helped build a more humane world for animals, helping to pass thousands of groundbreaking laws, rescuing hundreds of thousands of individual animals, and caring for and protecting millions more.\nTo address the global challenge of managing street animal populations, Humane World for Animals has developed Humane World Apps‚Äîa comprehensive global data management tool designed to improve dog and cat welfare programs through spay and neuter management, mass vaccination, and access to care. With support from the AWS Imagine Grant‚Äîa public grant program that provides both cash and Amazon Web Services (AWS) credits to registered nonprofits using cloud technology to advance their mission‚ÄîHumane World Apps is transforming how large-scale animal welfare programs operate worldwide.\nDuring the beta phase in Vadodara, India, Humane World Apps achieved a dog sterilization rate of up to 86% and a 60% reduction in dog-related complaints after app deployment. This technology aims to end the suffering of companion animals by improving program efficiency, supporting community health, and promoting innovative sustainable solutions to reduce human-dog conflict.\nIn this article, we will present how Humane World Apps is revolutionizing animal welfare management through cloud technology and data-driven solutions.\nFragmented data collection in global animal welfare The scale of the challenge in managing street animal populations is enormous. It is estimated that there are between 700 million and 1 billion dogs worldwide, including both owned and unowned dogs living in shelters, families, or freely in urban and rural environments. According to a MARS study, there are nearly 362 million stray dogs and cats in 20 countries. Communities and government agencies‚Äîespecially in developing economies‚Äîstruggle to find reasonable, humane, and accessible solutions to manage dog and cat populations, promote animal welfare, and build peaceful coexistence between companion animals and humans.\nGovernments are responsible for community health and safety, but many places still lack specific solutions and evidence-based programs. Additionally, many places still apply lethal methods to street animals. Long-term solutions including humane and effective sterilization, vaccination, and access to affordable veterinary services are key to improving animal welfare and reducing human-companion animal conflict in areas with stray animal problems.\nHistory shows that large-scale dog and cat welfare programs often struggle due to ineffective, fragmented data management and collection. The consequence is suboptimal results in controlling stray dog and cat populations and addressing community health issues.\nBuilding a comprehensive solution on AWS The goal of Humane World Apps is to provide a unified, high-tech platform that integrates data management systems for activities such as spay and neuter, mass vaccination, and veterinary clinic operations. The application includes both mobile applications and web interfaces, providing advanced features tailored to each program\u0026rsquo;s needs.\nThe mobile application aims to support field teams in collecting real-time data and recording capture locations to ensure dogs are returned to their original locations. The system is expected to track the entire care cycle, including pre- and post-surgical care, surgical data, and image identification to monitor individual animals as well as program outcomes. Users can expect the application to manage rabies vaccination programs through geo-fencing, GPS positioning, and automatic reporting features.\nSince Humane World for Animals is a nonprofit organization, the initial beta version of the application was built on a limited budget. Some developers were volunteers, and they designed and built the application architecture using free or low-cost services. With support from the AWS Imagine Grant (Amazon Web Services\u0026rsquo; grant program for nonprofits), the organization is now in the process of transitioning the beta application to AWS services to unify and simplify the technology system.\nCurrently, the application is designed to use a single Amazon Elastic Compute Cloud (Amazon EC2) server to host both production and testing environments. Additionally, database servers for production and testing environments use Amazon Relational Database Service (Amazon RDS). The application uses Amazon Simple Storage Service (Amazon S3) to store animal images. During the transition, the organization has begun using Amazon Simple Email Service (Amazon SES), and plans to leverage additional AWS tools in the future.\nTo ensure redundancy, the organization maintains an AWS instance in Ohio for cold standby. The production and testing environments in Northern Virginia currently use Cloudflare CDN, and will be migrated to Amazon CloudFront in the future. LucidScale is used to diagram the entire AWS architecture to help staff understand the system and support future operations.\nExpanding impact through cloud technology Support from the AWS Imagine Grant is helping Humane World for Animals transition from a distributed technology system to a unified system on AWS. The transition from beta architecture to AWS services is designed to promote global deployment capabilities and increase scalability as the team moves toward putting the application into production. The project will continue to advance the organization\u0026rsquo;s mission through results based on actual effectiveness.\nHumane World Apps is currently in the early stages of deployment in multiple countries, building on the success of the beta phase in India. The beta phase provided strong feedback on the application\u0026rsquo;s effectiveness, demonstrating that technology can change animal welfare outcomes when properly deployed and scaled.\nThe web application serves as an administrative center where program managers design and monitor operations, provide feedback to vaccination teams, generate reports, and manage data across multiple regions. This centralized approach allows for better coordination and more efficient resource allocation across global programs.\nAdvice for nonprofit technology deployment Humane World for Animals offers valuable advice for nonprofits starting technology projects. First, they strongly recommend leveraging resources from Amazon, especially guidance from their technical solution architects. Second, they emphasize staying focused on the ultimate goal‚Äîwhen facing difficulties, remember the project\u0026rsquo;s impact on the lives of street dogs. Finally, they emphasize the importance of implementing software development lifecycle processes early. Establishing source code control and error and requirement prioritization systems before starting is crucial, as implementing them midway consumed valuable time from the grant funding. Although the challenges were significant, the organization\u0026rsquo;s commitment to improving animal welfare worldwide drove them to overcome obstacles and build a scalable, effective solution.\nThe success of Humane World Apps demonstrates the transformative power of cloud technology in addressing global challenges. By leveraging AWS services and the AWS Imagine Grant, Humane World for Animals has created a platform that not only improves animal welfare outcomes but also serves as a model for other nonprofits looking to scale their impact through technology.\nAs the organization continues to expand its reach and refine its platform, the lessons learned from this implementation will undoubtedly benefit the broader animal welfare community. The combination of innovative technology, dedicated volunteers, and strategic partnerships has created a powerful tool for creating positive change in the lives of animals worldwide.\nThe future of animal welfare programs lies in data-driven approaches that can adapt to local needs while maintaining global standards. Humane World Apps represents a significant step forward in this direction, demonstrating how technology can be harnessed to create more effective, efficient, and humane solutions to complex global challenges.\nAuthors Stacy Stonich\nJules Marenghi\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/3.-translated-blogs/",
	"title": " Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Translated Blogs Technical blog translations and knowledge sharing content\n3.1 AWS Big Data Blog - OpenSearch Serverless Monitoring A comprehensive guide to implementing CloudWatch monitoring for Amazon OpenSearch Serverless collections. This technical article covers essential metrics, alert configurations, and best practices for maintaining optimal performance and reliability in serverless OpenSearch deployments. The guide includes detailed recommendations for monitoring key performance indicators, setting appropriate thresholds, and establishing proactive alerting systems to ensure seamless operation of OpenSearch Serverless workloads.\n3.2 Humane World for Animals - AWS Global Animal Welfare Programs A compelling case study showcasing how Humane World for Animals leverages AWS cloud technology to scale their global animal welfare initiatives. The organization developed Humane World Apps, a comprehensive data management platform that revolutionizes street animal population management through spay/neuter programs, mass vaccination campaigns, and veterinary care coordination. With AWS Imagine Grant support, they achieved remarkable results including 86% dog sterilization rates and 60% reduction in animal-related complaints during their beta deployment in India.\n3.3 AWS Quantum Computing - Pharmaceutical Research Collaboration An advanced technical exploration of AWS\u0026rsquo;s collaboration with Chugai Pharmaceutical Co. on quantum-inspired computational methods for drug discovery. This research focuses on cyclic peptide-protein docking using Quadratic Unconstrained Binary Optimization (QUBO) models and Constraint Programming approaches. The study demonstrates innovative applications of quantum computing principles in pharmaceutical research, comparing classical and quantum optimization techniques for molecular modeling and drug development processes.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/3-ai/",
	"title": "AI Workshop (Chatbot)",
	"tags": [],
	"description": "",
	"content": "This workshop provides a detailed guide on how to build a Product Consultation Chatbot using RAG (Retrieval-Augmented Generation) architecture on the AWS platform.\n1. System Architecture The system utilizes the following AWS services:\nAmazon Bedrock: Provides AI models (LLMs). Generation Model: Amazon Nova Lite (anthropic.claude-3-haiku-20240307-v1:0) for answering questions. Embedding Model: Cohere Embed Multilingual (cohere.embed-multilingual-v3) for data vectorization. Amazon RDS (PostgreSQL): Stores product data and vectors (using the Dbeaver extension). AWS Lambda: Intermediate logic processing function (Serverless backend). "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/4-frontend/4.3/",
	"title": "API Integration",
	"tags": [],
	"description": "",
	"content": "Step 1: Get Your API Gateway URL First of all you have to receive API GateWay url https://uwbxj9wfq6.execute-api.ap-southeast-1.amazonaws.com/dev Open your project folder in VS Code. Create a new file named api.js inside your src directory. Add the following JavaScript code: Step 2: Test the API in Postman Take the data from postman You should receive a status 200 response with JSON data Step 3: Deploy and Verify on S3 Visit your http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com Using data from Postman with https://uwbxj9wfq6.execute-api.ap-southeast-1.amazonaws.com/dev If you login and it show a notification like this the API Integration was successfully "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/3.-translated-blogs/blog1/",
	"title": "AWS Big Data Blog - OpenSearch Serverless Monitoring",
	"tags": [],
	"description": "",
	"content": "AWS Big Data Blog - OpenSearch Serverless Monitoring Guide to Setting Up Amazon OpenSearch Serverless Monitoring with CloudWatch by Urmila Iyer and Parth Shah on September 24, 2025 in Advanced (300), Amazon CloudWatch, Amazon OpenSearch Service, Monitoring and observability, Serverless, Technical How-to Permalink Comments Share\nAmazon OpenSearch Serverless simplifies the deployment and management of OpenSearch workloads by automatically scaling based on your usage patterns. This service considers key metrics such as shard utilization, storage consumption, and CPU usage while maintaining millisecond response times, with the simplicity of a serverless environment.\nWhile OpenSearch Serverless automatically handles scaling, implementing robust monitoring is still crucial for understanding usage patterns, optimizing costs, ensuring performance, and maintaining reliability. Proactive monitoring helps organizations detect critical issues with applications or infrastructure in real-time and quickly identify root causes.\nThis article is part of a series on monitoring Amazon OpenSearch services, focusing on OpenSearch Serverless workloads and deployments. In this article, we explore commonly used CloudWatch metrics and alerts for OpenSearch Serverless, guide you through the process of selecting appropriate metrics, setting proper thresholds, and configuring alerts. This guide will provide you with a comprehensive monitoring strategy that complements the serverless nature of your OpenSearch deployment while maintaining full operational observability.\nKey Benefits of CloudWatch Monitoring for OpenSearch Serverless Implementing CloudWatch monitoring for OpenSearch Serverless collections provides several key benefits:\nNear real-time performance monitoring: CloudWatch provides near real-time monitoring capabilities, allowing you to track the performance of OpenSearch Serverless collections during operation. This immediate observability enables quick detection of anomalies or performance issues, helping with timely response to potential problems. Efficient error diagnosis: You can quickly identify and resolve common errors without complex log analysis. For example, by monitoring ingestion request errors, you can proactively mitigate bulk indexing request failures. Proactive alert system: Use CloudWatch alert functionality combined with Amazon Simple Notification Service (Amazon SNS) to set up custom alerts. By defining specific thresholds for critical metrics, you can receive immediate notifications via email or SMS when your OpenSearch Serverless collections approach or exceed these limits. Comprehensive historical analysis: CloudWatch\u0026rsquo;s data retention capabilities enable in-depth historical analysis. This helps you identify long-term performance trends, recognize recurring patterns in resource usage, and optimize workload distribution based on historical insights. Solution Overview Understanding which metrics to monitor in OpenSearch Serverless helps optimize system performance and reliability. This guide explains the key metrics to monitor, their significance, how to determine appropriate thresholds, and step-by-step procedures for setting up alerts. Understanding these fundamentals will help you establish effective monitoring for OpenSearch Serverless collections and maintain optimal performance and reliability.\nPrerequisites Before getting started, you need the following:\nAn AWS account that provides access to AWS services. An OpenSearch Serverless collection. Recommended CloudWatch Metrics and Alerts for OpenSearch Serverless The table below summarizes key CloudWatch metrics for OpenSearch Serverless, including recommended alert thresholds, metric descriptions, and applicable workload types.\nCloudWatch metrics dashboard for OpenSearch Serverless\nSetting up monitoring alerts\nConfiguring CloudWatch alarms\nPerformance metrics visualization\nHistorical data analysis dashboard\nAuthors Urmila Iyer\nParth Shah\nRead the full article on AWS Blog\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/2.3/",
	"title": "Create Lambda Function to Handle DynamoDB Requests",
	"tags": [],
	"description": "",
	"content": "Objective Create a new Lambda Function to process requests to DynamoDB through API Gateway.\nSteps Download the backend file from here.\nStep 1: Create IAM Role Open IAM Console ‚Üí Roles ‚Üí Create role Select\nTrusted entity: AWS Service ‚Üí Lambda Attach the following permissions:\nAmazonDynamoDBFullAccess CloudWatchLogsFullAccess Set the role name: LambdaAPIAccessRole\nStep 2: Create Lambda Function Go to AWS Lambda ‚Üí Create function Select Author from scratch Function name: DynamoDB_API_Handler Runtime: Java 17 Choose IAM Role: LambdaAPIAccessRole Step 3: Deploy the JAR File Go to S3 ‚Üí Upload ‚Üí Add files\nUpload the jar file, then copy the Object URL Go to AWS Lambda ‚Üí Upload from S3\nPaste the object URL you copied Go to AWS Lambda ‚Üí Code ‚Üí Runtime settings ‚Üí Edit Set Handler:\norg.example.flyora_backend.handler.StreamLambdaHandler::handleRequest Step 4: Configure Lambda Function Go to AWS Lambda ‚Üí Configuration ‚Üí General configuration ‚Üí Edit Set Timeout: 1 min Go to AWS Lambda ‚Üí Configuration ‚Üí Environment variables ‚Üí Edit Add: Key: APP_JWT_SECRET Value: huntrotflyorateam!@ky5group5member Key: GHN_TOKEN; Value: 445c659d-5586-11f0-8c19-5aba781b9b65 "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/3-ai/3.3/",
	"title": "Create Logic for Lambda Function",
	"tags": [],
	"description": "",
	"content": "Create Logic for Lambda Function Implementation Steps 1. Access Lambda Service Go to AWS Management Console ‚Üí search for Lambda. First, go to the Layers section to create a layer so the Lambda function has the library to connect to PostgreSQL (psycopg2).\nAccess the GitHub link to download the appropriate Python version, here we use psycopg2-3.11: https://github.com/jkehler/awslambda-psycopg2\nAfter downloading, put the entire psycopg2-3.11 folder into a folder named (python), then zip that folder and name it (postgres-layer-3.11).\nClick Create layer, name the layer, upload the postgres-layer-3.11 zip file ‚Üí Create. 2. Create Lambda Functions ‚Üí Create function. Name the Lambda function and select runtime Python 3.11. In Additional configurations, check VPC, select the created VPC, select 1 private subnet for Subnet, and select Lambda-SG for Security group. After the Lambda function is created, we will go to the layer section to add the created layer. Select Custom layers, choose the created layer, select version 1 ‚Üí Add. Go to the Configuration tab, in General configuration, increase the timeout to 1 minute. In Permissions, add AmazonBedrockFullAccess and AWSLambdaVPCAccessExecutionRole. In Environment variables, add the following variables: DB_HOST, DB_NAME, DB_USER, DB_PASS (Enter your RDS information). After configuring, paste this Python code into the Lambda function and click Deploy.\nimport json import boto3 import psycopg2 import os # --- CONFIGURATION --- DB_HOST = os.environ.get(\u0026#39;DB_HOST\u0026#39;) DB_NAME = os.environ.get(\u0026#39;DB_NAME\u0026#39;) DB_USER = os.environ.get(\u0026#39;DB_USER\u0026#39;) DB_PASS = os.environ.get(\u0026#39;DB_PASS\u0026#39;) # Use ap-southeast-1 region bedrock = boto3.client(service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # --- 1. EMBEDDING FUNCTION (COHERE) --- def get_embedding(text): try: body = json.dumps({ \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34; }) response = bedrock.invoke_model( body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34; ) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except Exception as e: print(f\u0026#34;Embed Error: {e}\u0026#34;) return None # --- 2. SAFE METADATA HANDLING FUNCTION --- def format_product_info(metadata): \u0026#34;\u0026#34;\u0026#34; This function helps normalize data even if the CSV file is missing columns \u0026#34;\u0026#34;\u0026#34; if not metadata: return None # Get name (Prioritize common keys) name = metadata.get(\u0026#39;name\u0026#39;) or metadata.get(\u0026#39;product_name\u0026#39;) or metadata.get(\u0026#39;title\u0026#39;) or \u0026#34;Unnamed Product\u0026#34; # Get price (If not present, leave empty or \u0026#39;Contact\u0026#39;) price = metadata.get(\u0026#39;price\u0026#39;) or metadata.get(\u0026#39;gia\u0026#39;) or metadata.get(\u0026#39;display_price\u0026#39;) price_str = f\u0026#34;- Price: {price}\u0026#34; if price else \u0026#34;\u0026#34; # Get type (if available from smart import script) category = metadata.get(\u0026#39;type\u0026#39;) or \u0026#34;Product\u0026#34; # Create description string for AI to read # Example: \u0026#34;Bird Species: Parrot. - Price: 500k\u0026#34; ai_context = f\u0026#34;Category/Product: {name} ({category}) {price_str}\u0026#34; # Create object for Frontend display (Product Card) frontend_card = { \u0026#34;id\u0026#34;: metadata.get(\u0026#39;id\u0026#39;) or metadata.get(\u0026#39;product_id\u0026#39;), \u0026#34;name\u0026#34;: name, \u0026#34;price\u0026#34;: price if price else \u0026#34;Contact\u0026#34;, # Frontend will show \u0026#34;Contact\u0026#34; if no price \u0026#34;image\u0026#34;: metadata.get(\u0026#39;image_url\u0026#39;) or metadata.get(\u0026#39;link_anh\u0026#39;) or \u0026#34;\u0026#34;, # Image can be empty \u0026#34;type\u0026#34;: category # To let frontend know if this is a bird or a toy } return ai_context, frontend_card # --- 3. MAIN HANDLER --- def lambda_handler(event, context): print(\u0026#34;Event:\u0026#34;, event) try: # Parse Input if \u0026#39;body\u0026#39; in event: try: body_data = json.loads(event[\u0026#39;body\u0026#39;]) if isinstance(event[\u0026#39;body\u0026#39;], str) else event[\u0026#39;body\u0026#39;] except: body_data = {} else: body_data = event user_question = body_data.get(\u0026#39;question\u0026#39;, \u0026#39;\u0026#39;) if not user_question: return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Missing question\u0026#39;)} # A. Create Vector q_vector = get_embedding(user_question) if not q_vector: return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Vector creation error\u0026#39;)} # B. Search DB conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS) cur = conn.cursor() # Retrieve metadata for processing sql = \u0026#34;\u0026#34;\u0026#34; SELECT content, metadata FROM knowledge_base ORDER BY embedding \u0026lt;=\u0026gt; %s LIMIT 3 \u0026#34;\u0026#34;\u0026#34; cur.execute(sql, (json.dumps(q_vector),)) results = cur.fetchall() cur.close(); conn.close() # C. Process Results (MOST IMPORTANT) ai_contexts = [] frontend_products = [] for row in results: raw_content = row[0] # Original content at import raw_metadata = row[1] # JSON metadata # Call normalization function ai_text, card_data = format_product_info(raw_metadata) if ai_text: # Merge original content + identification content to be sure ai_contexts.append(f\u0026#34;{ai_text}. Details: {raw_content}\u0026#34;) frontend_products.append(card_data) # If nothing is found if not ai_contexts: final_answer = \u0026#34;Sorry, the shop currently cannot find matching information in the database.\u0026#34; else: # D. Send to LLM (Claude 3 Haiku) context_str = \u0026#34;\\n---\\n\u0026#34;.join(ai_contexts) system_prompt = ( \u0026#34;You are a consultant for the Bird Shop, named \u0026#39;Smart Parrot\u0026#39;. \u0026#34; \u0026#34;Style: Friendly, Polite but Concise.\\n\\n\u0026#34; \u0026#34;HANDLING INSTRUCTIONS (PRIORITIZE IN ORDER):\\n\u0026#34; \u0026#34;1. SOCIAL INTERACTION: If the customer just says hello (Hello, Hi...) or thanks: \u0026#34; \u0026#34;-\u0026gt; Greeting back warmly and ask what they are looking for. DO NOT list products unless asked.\\n\u0026#34; \u0026#34;2. PRODUCT CONSULTATION: When customer asks about goods:\\n\u0026#34; \u0026#34;-\u0026gt; Answer concisely: Product Name + Price + Stock status (if any).\\n\u0026#34; \u0026#34;-\u0026gt; Do not describe in flowery detail unless asked specifically \u0026#39;how is it\u0026#39;.\\n\u0026#34; \u0026#34;-\u0026gt; If information is not in Context, say \u0026#39;Sorry, the shop doesn\u0026#39;t have this item yet\u0026#39;.\\n\u0026#34; ) user_msg = f\u0026#34;Reference Information:\\n{context_str}\\n\\nQuestion: {user_question}\u0026#34; # Call Claude 3 Haiku claude_body = json.dumps({ \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;system\u0026#34;: system_prompt, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_msg}] }) try: model_res = bedrock.invoke_model( body=claude_body, modelId=\u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34; ) res_json = json.loads(model_res[\u0026#39;body\u0026#39;].read()) final_answer = res_json[\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;] except Exception as e: print(f\u0026#34;LLM Call Error: {e}\u0026#34;) final_answer = \u0026#34;Sorry, the AI system is busy, please try again later.\u0026#34; # E. Return Result return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;POST\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#34;answer\u0026#34;: final_answer, \u0026#34;products\u0026#34;: frontend_products # Frontend uses this to draw UI }) } except Exception as e: print(f\u0026#34;System Error: {e}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} 3. Integrate into Team\u0026rsquo;s API Gateway Access API Gateway Service. Select the API created by the Backend. Select Create resource. Resource name chatbot and check CORS. Select the chatbot resource and click Create method. Method type select POST, check Lambda proxy integration. Select the created Lambda Function (or VPC based on original context), then click Create method. After configuration is complete, click Deploy API. "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/2.4/",
	"title": "Create API Gateway and Integrate with Lambda",
	"tags": [],
	"description": "",
	"content": "Objective Connect AWS API Gateway with a Lambda Function to create a RESTful endpoint that allows accessing data stored in DynamoDB.\nImplementation Steps 1. Access API Gateway Go to AWS Console ‚Üí API Gateway Click Create API Select REST API (Build) Configure: Create new API: New API API name: FlyoraAPI Endpoint type: Regional Click Create API 2. Create Resources and Methods In the sidebar, select\nActions ‚Üí Create Resource Resource Name: api Click Create Resource Select /api ‚Üí Actions ‚Üí Create Resource Configure the resource: Resource path: /api/ Resource Name: v1 Click Create Resource Create a proxy resource under /api Check Proxy resource Resource path: /api/ Resource Name: {proxy+} Click Create Resource Select /v1 ‚Üí Actions ‚Üí Create Resource Configure:\nCheck Proxy resource Resource path: /api/v1/ Resource Name: {myProxy+} Click Create Resource Enable CORS for all resources Under OPTIONS ‚Üí Integration response ‚Üí Header Mappings, ensure the headers below exist:\nAccess-Control-Allow-Origin: * Access-Control-Allow-Headers:\nContent-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token Access-Control-Allow-Methods:\nDELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT 3. Integrate with Lambda After creating /api/v1/{myProxy+}, the ANY method appears: Select ANY ‚Üí Integration request ‚Üí Edit Attach Lambda: Integration type: Lambda Function Check Lambda proxy integration Lambda Region: ap-southeast-1 (Singapore) Lambda Function: select your Lambda_API_Handler 4. Deploy API Select Actions ‚Üí Deploy API Deployment stage: New stage Stage name: dev Description: Development stage for Lambda API Click Deploy After deployment, you will receive an Invoke URL in the format:\nhttps://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Event 1 :: Internship Report Event Report \u0026ldquo;AWS Cloud Day Vietnam\u0026rdquo; Purpose of the Event Full-day conference focused on reimagining business with Cloud \u0026amp; GenAI, particularly Migration \u0026amp; Modernization strategies.\nSpeakers List Morning Plenary H.E. Pham Duc Long - Vice Minister of Science and Technology H.E Marc E. Knapper - US Ambassador to Vietnam Eric Yeo - Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner - CEO, Techcombank Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network Jaime Valles - Vice President, General Manager Asia Pacific and Japan, AWS Track 4: Migration \u0026amp; Modernization Hieu Hoang - Senior Manager, Solutions Architect, AWS Chinh Hoang Minh - Senior Solutions Architect, AWS Harry Nguyen - Director of Engineering, Techcombank Quan Phuong - Solutions Architect, AWS Le Hai Duy - Solution Expert, VPBank Nguyen Thai Binh - Head of DevOps, Renova Cloud Key Highlights Panel Discussions Reimagining Business with Cloud \u0026amp; GenAI: Leadership Perspectives on People, Culture, and Innovation Application Modernization: Accelerating Business Transformation Microservice Architecture Techcombank\u0026rsquo;s journey from monolithic to microservices architecture demonstrated significant improvements in scalability and maintainability. The session covered containerization strategies using AWS ECS and EKS.\nDomain-Driven Design (DDD) VPBank\u0026rsquo;s implementation of DDD principles in their cloud modernization project showed how proper domain modeling can accelerate development cycles and improve code quality.\nEvent-Driven Architecture Renova Cloud\u0026rsquo;s presentation on integrating event-driven patterns with AWS EventBridge and Lambda functions provided practical insights into building resilient, loosely-coupled systems.\nCompute Evolution The evolution from traditional EC2 instances to serverless computing with Lambda and containerized workloads on Fargate represents a paradigm shift in how we approach application deployment.\nAmazon Q Developer AI-powered development assistant that helps with code generation, debugging, and optimization. The demo showed 40% improvement in development velocity when integrated into existing workflows.\nLessons Learned Design Thinking GenAI integration requires careful consideration of data privacy and security User-centric design principles apply equally to AI-powered applications Iterative development approach works best for AI/ML implementations Technical Architecture Microservices enable better GenAI integration at service boundaries Event-driven architecture supports real-time AI processing workflows Proper observability is crucial for AI-powered systems Modernization Strategy Start with pilot projects to validate GenAI use cases Invest in team training and change management Establish clear governance frameworks for AI implementations Practical Applications Implementing automated code review using Amazon Q Developer Setting up CI/CD pipelines with AI-powered testing Deploying intelligent monitoring solutions with CloudWatch Insights Creating event-driven architectures for real-time data processing Personal Experience \u0026amp; Reflections The conference provided invaluable insights into how leading Vietnamese companies are leveraging AWS and GenAI for digital transformation. The networking opportunities with industry leaders and the hands-on demonstrations of cutting-edge technologies made this event particularly enriching. The real-world case studies from Techcombank and VPBank offered practical perspectives on overcoming common modernization challenges.\nEvent Photos Opening ceremony with government officials and AWS leadership\nMigration \u0026amp; Modernization track sessions\nFree gift\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Event 2 :: Internship Report Event Report \u0026ldquo;AI-Driven Development Life Cycle: Reimagining Software Engineering\u0026rdquo; Purpose of the Event Exploring AI-Driven Development using Amazon Q Developer and Kiro to revolutionize software engineering practices and development workflows.\nSpeakers List Instructors Toan Huynh - AI-Driven Development Life Cycle \u0026amp; Amazon Q Developer Expert My Nguyen - Kiro Platform Specialist Coordinators Diem My - Event Coordinator Dai Truong - Technical Coordinator Dinh Nguyen - Workshop Coordinator Key Highlights Session Schedule 2:00 PM - 2:15 PM: Welcome \u0026amp; Introduction 2:15 PM - 3:30 PM: AI-Driven Development Life Cycle overview and Amazon Q Developer demonstration 3:30 PM - 3:45 PM: Networking Break 3:45 PM - 4:30 PM: Kiro platform demonstration and hands-on session AI-Driven Development Lifecycle Comprehensive overview of how AI transforms traditional software development processes, from requirements gathering to deployment and maintenance.\nAmazon Q Developer Integration Hands-on demonstration of Amazon Q Developer\u0026rsquo;s capabilities in code generation, debugging, testing, and documentation automation.\nKiro Platform Architecture Exploration of Kiro\u0026rsquo;s AI-powered development environment and its integration with existing development workflows.\nDevelopment Velocity Optimization Techniques for accelerating development cycles using AI-assisted coding, automated testing, and intelligent code review processes.\nAutomated Code Generation Practical examples of AI-generated code snippets, unit tests, and documentation using both Amazon Q Developer and Kiro platforms.\nLessons Learned AI Integration Strategy Start with pilot projects to validate AI tool effectiveness Establish coding standards for AI-generated code Implement proper review processes for AI-assisted development Technical Implementation Amazon Q Developer reduces coding time by 35-50% Kiro platform enhances code quality through intelligent suggestions AI tools require proper training data and context for optimal performance Development Workflow Enhancement Seamless integration with existing IDEs and CI/CD pipelines Real-time code analysis and optimization recommendations Automated documentation generation improves project maintainability Practical Applications Implementing Amazon Q Developer in VS Code and IntelliJ environments Setting up Kiro platform for team collaboration and code review Creating AI-assisted testing frameworks for automated quality assurance Developing intelligent code completion and refactoring workflows Personal Experience \u0026amp; Reflections The workshop provided hands-on experience with cutting-edge AI development tools. The live demonstrations of Amazon Q Developer\u0026rsquo;s code generation capabilities were particularly impressive, showing real-time solutions to complex programming challenges. Kiro\u0026rsquo;s collaborative features demonstrated how AI can enhance team productivity while maintaining code quality standards.\nEvent Photos Welcome session and introduction to AI-Driven Development\nToan Huynh demonstrating Amazon Q Developer capabilities\nMy Nguyen presenting Kiro platform features\nParticipants engaging in hands-on AI-assisted coding exercises\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Event 3 :: Internship Report Event Report \u0026ldquo;Data Resiliency in a Cloud-first World - AWS + Commvault\u0026rdquo; Purpose of the Event Comprehensive exploration of data resiliency, cyber resilience, and modern backup strategies in cloud-first environments, featuring AWS and Commvault solutions.\nSpeakers List Main Speaker Paul Haverfield - ASEAN Storage Lead., WW Specialist Organization AWS Experts AWS Solutions Architects - Data Protection and Backup Specialists AWS Security Team - Cyber Resilience Experts Commvault Representatives Commvault Technical Specialists - Cloud Data Management Elastio Integration Experts - AWS Backup Solutions Key Highlights Event Schedule 14/10/2025\n8:30 AM: Lite breakfast and Registration, networking with colleagues 9:30 AM: Event keynote - The Resiliency requirement in 2025 versus traditional protection in 2015 10:00 AM: Modernize Cyber Resilience on AWS: Immutable Backups, Isolated Testing, Rapid Recovery 10:30 AM: Refreshment break 10:50 AM: How to get started with data resiliency in the cloud 11:25 AM: Detect, Respond, Recover and Govern with AWS Backup and Elastio 12:00 PM: Lunch break 1:00 PM: Two Afternoon hands-on workshops (choose between AWS data services or Elastio with AWS Backup) 4:00 PM: Fun event quiz and next step message Modern Data Resiliency Evolution from traditional backup solutions to cloud-native resiliency strategies, emphasizing immutable backups and automated recovery processes.\nCyber Resilience Framework Comprehensive approach to protecting against ransomware and cyber threats using AWS security services and Commvault\u0026rsquo;s advanced protection capabilities.\nAWS Backup Integration Deep dive into AWS Backup service capabilities, cross-region replication, and integration with third-party solutions like Elastio for enhanced data protection.\nImmutable Storage Solutions Implementation of immutable backup strategies using AWS S3 Object Lock and Glacier for long-term data retention and compliance requirements.\nAutomated Recovery Processes Demonstration of automated disaster recovery workflows, including isolated testing environments and rapid recovery procedures for business continuity.\nLessons Learned Data Protection Strategy Transition from traditional backup to cloud-native resiliency requires comprehensive planning Immutable backups are essential for ransomware protection Regular testing of recovery procedures ensures business continuity Technical Implementation AWS Backup provides centralized management for multiple AWS services Elastio enhances AWS Backup with advanced monitoring and analytics Cross-region replication ensures geographic disaster recovery Governance and Compliance Automated compliance reporting reduces administrative overhead Data lifecycle management optimizes storage costs Regular audit trails maintain regulatory compliance Practical Applications Implementing AWS Backup for EC2, RDS, and EFS resources Setting up Elastio integration for enhanced backup monitoring Creating automated disaster recovery runbooks Establishing data retention policies for compliance requirements Personal Experience \u0026amp; Reflections The event provided valuable insights into modern data protection strategies in cloud environments. The hands-on workshops were particularly beneficial, offering practical experience with AWS Backup and Elastio integration. The networking sessions allowed for knowledge sharing with industry peers facing similar data resiliency challenges.\nEvent Photos Workshop with Backup services\nSharings about backup\nDemonstration of Elastio with AWS Backup integration\nNIST FRAMEWORK\nLunch break and networking with delicious food and refreshments\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Event 4 :: DevOps on AWS Event Details Date: Monday, November 17, 2025 Time: 8:30 AM ‚Äì 5:00 PM Location: AWS Morning Session (8:30 AM ‚Äì 12:00 PM) 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; DevOps Mindset Recap of AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 ‚Äì 10:30 AM | AWS DevOps Services ‚Äì CI/CD Pipeline Source Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation Demo: Full CI/CD pipeline walkthrough 10:30 ‚Äì 10:45 AM | Break 10:45 AM ‚Äì 12:00 PM | Infrastructure as Code (IaC) AWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Lunch Break (12:00 ‚Äì 1:00 PM) Self-arranged\nAfternoon Session (1:00 ‚Äì 5:00 PM) 1:00 ‚Äì 2:30 PM | Container Services on AWS Docker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 ‚Äì 2:45 PM | Break 2:45 ‚Äì 4:00 PM | Monitoring \u0026amp; Observability CloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, and on-call processes 4:00 ‚Äì 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Deployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: Startups and enterprise DevOps transformations 4:45 ‚Äì 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up DevOps career pathways AWS certification roadmap Event Photos DevOps on AWS Workshop\nCI/CD Pipeline demonstration\nInteractive Kahoot quiz session\nWorkshop resources and materials\nKey Takeaways Comprehensive understanding of AWS DevOps services and CI/CD pipelines Hands-on experience with Infrastructure as Code using CloudFormation and CDK Practical knowledge of container services (ECS, EKS, ECR) Best practices for monitoring, observability, and incident management "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "Event 5 :: AWS GenAI Game Day - Secret Agent(ic) Unicorns Event Overview Join us for an immersive and collaborative learning exercise to develop practical skills in implementing AI-assisted solutions on AWS. Dive into a live AWS environment where you will enhance various workloads with cutting-edge AI capabilities. You will leverage and build skills in Amazon Bedrock, foundation models, AI agents, intelligent automation, and ML-powered operations. Face challenges that blend traditional cloud architecture with the power of AI! Whether you are an AI enthusiast or just getting started and looking to build your AI skillset, this GameDay will put your AI skills to the ultimate test. This session features integrated use cases with Dynatrace.\nEvent Details Time: 2pm-5pm Friday 21 Nov 2025 Location: AWS Event Hall, L26, Bitexco Tower, 2 Hai Trieu, HCMC Team Registration: https://pulse.aws/survey/Z8M89UUP (3-5 members per team) AWS Services Used Amazon Bedrock (FMs, Knowledge Bases, Guardrails) Amazon Bedrock AgentCore (Runtime, Memory, Code Interpreter, Observability) Strands Agents MCP Amazon DynamoDB Amazon Q Developer for CLI Amazon OpenSearch Serverless Target Audience Secret Agent(ic) AI GameDay has elements that should appeal to a broad range of job functions and customers with all levels of AWS experience, including data scientists, ML practitioners, architects, developers and operations staff. Players should be at least familiar in navigating AWS Console. Players with ML experience and AWS Machine Learning Specialty Certification should be able to follow along some advanced concepts. While basic programming skills might be handy in a team, they are not required to participate in this gameday. We encourage to mix the team with players of different levels of ML experiences to promote collaboration.\nDifficulty Ideally each team should have around 3-5 members of varying skill levels, for example 1-2 Expert 1-2 Intermediate and 1-2 Beginner.\nDuration \u0026amp; Agenda Plan 3 hours for the event. Plenty of content to explore. The team with the highest points wins. Here is a high-level agenda:\nIntro Presentation and AWS account logins - 30 minutes Game playtime (breaks included) - 120 minutes Closing (Distribute survey link, announce top 3 winning teams, recap learning experience, pictures) - 30 minutes Event Photos AWS GenAI Game Day - Secret Agent(ic) Unicorns\nSpeaker Dai Truong\nTeams working together on AI challenges\nParticipants solving AI-powered challenges\nCelebrating the winning teams\nKey Takeaways Hands-on experience with Amazon Bedrock and AI agents Practical skills in implementing AI-assisted solutions Collaborative problem-solving with AI and cloud architecture Understanding of ML-powered operations and intelligent automation "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/event6/",
	"title": "Event 6",
	"tags": [],
	"description": "",
	"content": "Event 6 :: AWS Well-Architected Security Pillar Event Details Date: Friday, November 29, 2025 Time: 8:30 AM ‚Äì 12:00 PM (Morning Only) Location: AWS Vietnam Office 8:30 ‚Äì 8:50 AM | Opening \u0026amp; Security Foundation Security Pillar role in Well-Architected Core principles: Least Privilege ‚Äì Zero Trust ‚Äì Defense in Depth Shared Responsibility Model Top threats in Vietnam cloud environment ‚≠ê Pillar 1 ‚Äî Identity \u0026amp; Access Management 8:50 ‚Äì 9:30 AM | Modern IAM Architecture IAM: Users, Roles, Policies ‚Äì avoid long-term credentials IAM Identity Center: SSO, permission sets SCP \u0026amp; permission boundaries for multi-account MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access ‚≠ê Pillar 2 ‚Äî Detection 9:30 ‚Äì 9:55 AM | Detection \u0026amp; Continuous Monitoring CloudTrail (org-level), GuardDuty, Security Hub Logging at all layers: VPC Flow Logs, ALB/S3 logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules) 9:55 ‚Äì 10:10 AM | Coffee Break ‚≠ê Pillar 3 ‚Äî Infrastructure Protection 10:10 ‚Äì 10:40 AM | Network \u0026amp; Workload Security VPC segmentation, private vs public placement Security Groups vs NACLs: application model WAF + Shield + Network Firewall Workload protection: EC2, ECS/EKS security basics ‚≠ê Pillar 4 ‚Äî Data Protection 10:40 ‚Äì 11:10 AM | Encryption, Keys \u0026amp; Secrets KMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store ‚Äî rotation patterns Data classification \u0026amp; access guardrails ‚≠ê Pillar 5 ‚Äî Incident Response 11:10 ‚Äì 11:40 AM | IR Playbook \u0026amp; Automation IR lifecycle according to AWS Playbook: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response using Lambda/Step Functions 11:40 ‚Äì 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of 5 pillars Common pitfalls \u0026amp; Vietnam enterprise reality Security learning roadmap (Security Specialty, SA Pro) Event Photos AWS Well-Architected Security Pillar Workshop\nSecurity Groups configuration\nKMS encryption workflow\nIncident Response playbook\nHands-on security implementation\nEvent resources and materials\nKey Takeaways Comprehensive understanding of AWS Security Pillar and 5 core areas Practical IAM architecture and access management strategies Detection and monitoring best practices with AWS security services Infrastructure and data protection implementation patterns Incident response playbooks and automation techniques "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/event7/",
	"title": "Event 7",
	"tags": [],
	"description": "",
	"content": "Event 7 :: AI/ML/GenAI on AWS Event Details Date: Saturday, November 15, 2025 Time: 8:30 AM ‚Äì 12:00 PM Location: AWS Vietnam Office 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker ‚Äì End-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 ‚Äì 10:45 AM | Coffee Break 10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan ‚Äì comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock 12:00 PM | Lunch Break Self-arranged\nEvent Photos AWS AI/ML new services\nBedrock Agent Core Services architecture\nRetrieval-Augmented Generation implementation\nPipecat framework introduction\nPipecat hands-on demonstration\nKey Takeaways Comprehensive understanding of AWS AI/ML services ecosystem Hands-on experience with Amazon SageMaker for ML workflows Practical knowledge of Generative AI with Amazon Bedrock RAG architecture and implementation patterns Prompt engineering techniques and best practices Building AI agents with multi-step workflows "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/4.-events/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Events Participated Event 1 Event Name: AWS Cloud Day Vietnam Time: 7:35 AM - 5:15 PM on September 18, 2025 Location: 36th Floor (track 2 HCMC), Bitexco Building, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City Role in Event: Participant Description: Full-day conference on Cloud \u0026amp; GenAI with sessions on Migration \u0026amp; Modernization Outcomes: Gained insights into GenAI applications in modernization and DevOps lifecycle on AWS Event 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering Time: 2:00 PM - 4:30 PM on October 3, 2025 Location: AWS Event Hall, L26 Bitexco Tower, HCMC Role in Event: Participant Description: Workshop on AI-Driven Development using Amazon Q Developer and Kiro platform Outcomes: Gained hands-on experience with AI-powered development tools and automated code generation Event 3 Event Name: Data Resiliency in a Cloud-first World - AWS + Commvault Time: 8:30 AM - 4:00 PM on October 15, 2025 Location: AWS Event Center, Ho Chi Minh City Role in Event: Participant Description: Full-day event on data resiliency, cyber resilience, and backup strategies in cloud environments Outcomes: Enhanced understanding of AWS Backup, Elastio integration, and modern data protection strategies Event 4 Event Name: DevOps on AWS Time: 8:30 AM - 5:00 PM on November 17, 2025 Location: AWS Vietnam Office Role in Event: Participant Description: Full-day workshop on DevOps practices, CI/CD pipelines, Infrastructure as Code, and container services Outcomes: Comprehensive understanding of AWS DevOps services, hands-on experience with CloudFormation, CDK, and container orchestration Event 5 Event Name: AWS GenAI Game Day - Secret Agent(ic) Unicorns Time: 2:00 PM - 5:00 PM on November 21, 2025 Location: AWS Event Hall, L26, Bitexco Tower, HCMC Role in Event: Participant Description: Immersive GameDay focused on AI-assisted solutions, Amazon Bedrock, AI agents, and ML-powered operations Outcomes: Hands-on experience with Amazon Bedrock, AI agents, and collaborative problem-solving with AI and cloud architecture Event 6 Event Name: AWS Well-Architected Security Pillar Time: 8:30 AM - 12:00 PM on November 29, 2025 Location: AWS Vietnam Office Role in Event: Participant Description: Morning workshop covering 5 security pillars: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response Outcomes: Comprehensive understanding of AWS security best practices, IAM architecture, and incident response playbooks Event 7 Event Name: AI/ML/GenAI on AWS Time: 8:30 AM - 12:00 PM on November 15, 2025 Location: AWS Vietnam Office Role in Event: Participant Description: Morning workshop on AWS AI/ML services, Amazon SageMaker, and Generative AI with Amazon Bedrock Outcomes: Hands-on experience with SageMaker, understanding of RAG architecture, prompt engineering techniques, and building AI agents "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/4-frontend/",
	"title": "Frontend Workshop (UI)",
	"tags": [],
	"description": "",
	"content": "Frontend Hosting and API Integration on AWS In this workshop, you will learn how to deploy a frontend web application on AWS and connect it to a backend API hosted via Amazon API Gateway.\nThis activity combines frontend hosting and API integration, showing how AWS services can support interactive, serverless web applications.\nAmazon S3 ‚Äì Stores and serves your static web assets (HTML, CSS, JS).\nAmazon CloudFront ‚Äì Distributes your website globally with HTTPS and low latency.\nAmazon API Gateway ‚Äì Exposes backend API endpoints that your frontend can call.\nThis workshop demonstrates how to connect a static website to a backend API through API Gateway, creating a complete serverless web architecture that enables real-time data interaction between frontend and backend.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/5-cicd/",
	"title": "CI/CD Automation",
	"tags": [],
	"description": "",
	"content": "AWS CodeBuild Setup Guide for Flyora Frontend This guide covers CI/CD setup for the Frontend Repository (React).\nThis guide walks you through setting up AWS CodeBuild and CodePipeline for the Flyora React frontend application.\nPrerequisites AWS Account with appropriate permissions GitHub repository: QuangHieu-lab/Flyora-shop buildspec.yml file in the repository root (see below) AWS Region: Singapore (ap-southeast-1) (or your preferred region) S3 bucket for hosting static files CloudFront distribution (optional, for CDN) Required: buildspec.yml File Create a buildspec.yml file in the root of your repository with this content:\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Installing dependencies on `date`\u0026#34; - npm ci - echo \u0026#34;Running linter...\u0026#34; - npm run lint --if-present || echo \u0026#34;No lint script configured\u0026#34; build: commands: - echo \u0026#34;Running tests on `date`\u0026#34; - npm test -- --watchAll=false --passWithNoTests --coverage || echo \u0026#34;Tests completed\u0026#34; - echo \u0026#34;Building application on `date`\u0026#34; - npm run build - echo \u0026#34;Build completed on `date`\u0026#34; post_build: commands: - echo \u0026#34;Post-build phase started on `date`\u0026#34; - echo \u0026#34;Checking if build directory exists...\u0026#34; - ls -la build/ - echo \u0026#34;Build artifacts created successfully\u0026#34; - echo \u0026#34;Build completed successfully - artifacts ready for deployment\u0026#34; - echo \u0026#34;Use CodePipeline Deploy stage for S3 deployment\u0026#34; - echo \u0026#34;CloudFront invalidation will be handled by CodePipeline\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: build name: BuildArtifact discard-paths: yes cache: paths: - \u0026#39;/root/.npm/**/*\u0026#39; - \u0026#39;node_modules/**/*\u0026#39; Key Points:\nUses npm ci for faster, reliable installs Runs linter if configured Runs tests with coverage Builds React application Deployment handled by CodePipeline Deploy stage (not in buildspec) Caches npm and node_modules Step 1: Create S3 Bucket for Hosting Before setting up CodeBuild, create an S3 bucket to host your React application:\nGo to S3 console Click \u0026ldquo;Create bucket\u0026rdquo; Bucket name: flyora-frontend-hosting Region: Singapore (ap-southeast-1) Uncheck \u0026ldquo;Block all public access\u0026rdquo; (for static website hosting) Enable \u0026ldquo;Static website hosting\u0026rdquo; in bucket properties Set Index document: index.html Set Error document: index.html Step 2: Navigate to CodeBuild Open your browser and go to: https://console.aws.amazon.com/codesuite/codebuild/projects Sign in to your AWS account Ensure you\u0026rsquo;re in the Singapore (ap-southeast-1) region Click \u0026ldquo;Create build project\u0026rdquo; Step 3: Project Configuration Field Value Project name flyora-frontend-build Description Build project for Flyora React frontend Build badge Leave unchecked Step 4: Source Configuration Field Value Source provider GitHub Repository Repository in my GitHub account GitHub repository https://github.com/QuangHieu-lab/Flyora-shop | Source version | Leave blank | | Git clone depth | 1 | | Primary source webhook events | ‚ö†Ô∏è UNCHECK this |\nStep 5: Environment Configuration Section Field Value Provisioning Provisioning model On-demand Environment image Managed image Operating system Amazon Linux Runtime(s) Standard Image Latest (e.g., aws/codebuild/amazonlinux2-x86_64-standard:5.0) Service role Service role New service role Step 6: Environment Variables Add these environment variables in CodeBuild:\nName Value Type AWS_S3_BUCKET flyora-frontend-hosting Plaintext CLOUDFRONT_DISTRIBUTION_ID Your CloudFront distribution ID (if using) Plaintext REACT_APP_API_URL Your backend API URL Plaintext Step 7: Buildspec and Logs Buildspec:\nBuild specifications: Use a buildspec file Buildspec name: Leave blank Logs:\nCloudWatch logs: ‚úÖ Enable Group name: /aws/codebuild/flyora-frontend Step 8: IAM Permissions The CodeBuild service role needs permissions to access S3 and CloudFront. Add this policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::flyora-frontend-hosting\u0026#34;, \u0026#34;arn:aws:s3:::flyora-frontend-hosting/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 9: Create CodePipeline Go to CodePipeline console Click \u0026ldquo;Create pipeline\u0026rdquo; Pipeline name: flyora-frontend-pipeline Source stage: GitHub (Version 2) Build stage: Select flyora-frontend-build Deploy stage: Add S3 deploy action Action provider: Amazon S3 Bucket: flyora-frontend-hosting Extract file before deploy: ‚úÖ Checked Testing After pipeline creation:\nMake a change in your React repository Commit and push to GitHub Pipeline automatically triggers Check S3 bucket for updated files Access your website via S3 endpoint or CloudFront URL Troubleshooting Issue: Build fails with \u0026ldquo;npm: command not found\u0026rdquo; Solution: Ensure runtime-versions: nodejs: 18 is set in buildspec.yml\nIssue: S3 sync permission denied Solution: Check IAM role has S3 permissions (see Step 8)\nIssue: Website shows old content Solution:\nClear browser cache Invalidate CloudFront cache if using CDN Check S3 bucket has latest files Quick Reference Resource Value Pipeline Name flyora-frontend-pipeline Build Project Name flyora-frontend-build S3 Bucket flyora-frontend-hosting Region Singapore (ap-southeast-1) Source GitHub (QuangHieu-lab/Flyora-shop) Buildspec buildspec.yml (in repo root) Logs CloudWatch: /aws/codebuild/flyora-frontend Cost Estimate Item Cost CodePipeline $1/month per active pipeline CodeBuild (Free Tier) 100 build minutes/month for 12 months S3 Storage ~$0.023/GB/month S3 Requests Minimal for static hosting CloudFront Free tier: 1TB data transfer/month Monthly (estimated) $1-3/month Summary ‚úÖ Frontend CI/CD Pipeline Configured!\nAccomplished:\n‚úÖ CodeBuild project for React application ‚úÖ CodePipeline for automated workflow ‚úÖ GitHub integration with automatic triggers ‚úÖ Automatic deployment to S3 ‚úÖ Optional CloudFront CDN integration Your pipeline now:\nAutomatically detects code changes in GitHub Builds React application with npm Deploys static files to S3 Serves website via S3 or CloudFront "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/2.5/",
	"title": "Testing API using Postman",
	"tags": [],
	"description": "",
	"content": "Objective Test the API Gateway REST endpoint integrated with the Lambda Function to verify the data operations performed on DynamoDB.\nDownload and install Postman before starting this section.\n1. Update Authorization Settings Go to AWS Console ‚Üí API Gateway Select FlyoraAPI Navigate to\n/api/v1/{myProxy+} ‚Üí ANY ‚Üí Method request ‚Üí Edit Set Authorization to AWS_IAM(Note: Only enable when testing with Postman, then remember to turn it off afterwards) 2. Create an Access Key Go to AWS Console ‚Üí IAM ‚Üí Users Click Create User Set username: test Confirm user creation Open the test user ‚Üí Security credentials ‚Üí Create access key Choose Local code Copy the Access key and Secret access key Testing GET Request Open Postman\nChoose GET\nEnter URL:https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/product/1\nHeaders tab:\nKey: Content-Type | Value: application/json Authorization tab:\nType: AWS Signature Enter AccessKey Enter SecretKey AWS Region: ap-southeast-1 Service Name: execute-api Click Send\nResult: Returns the list of Items from the reviews table.\nTesting POST Request Choose POST\nURL:https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/submit\nBody ‚Üí raw ‚Üí JSON\n{ \u0026#34;customerId\u0026#34;: 2, \u0026#34;rating\u0026#34;: 4, \u0026#34;comment\u0026#34;: \u0026#34;Chim ƒÉn ngon v√† vui v·∫ª!\u0026#34;, \u0026#34;customerName\u0026#34;: \u0026#34;Nguy·ªÖn VƒÉn B\u0026#34; } Click Send\nResult: Inserts a new Item into the Review table.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Deploying the Flyora E-commerce System on AWS Overview In this workshop, you will deploy the core components of the Flyora platform using a Serverless architecture on AWS.\nThe objective is to build a system that is scalable, cost-efficient, and easy to maintain.\nComponents to be deployed:\nFrontend: Store \u0026amp; deliver UI via S3 + CloudFront Backend API: Handle business logic with API Gateway + AWS Lambda Database: Manage product / order data using DynamoDB + S3 User Authentication: Implemented via Amazon Cognito Chatbot: Product consultation assistant integrated into UI (handled by AI Team) The workshop is divided into group roles for parallel development: Backend (BE), AI (Chatbot), and Frontend (FE).\nSystem Architecture Workshop Content Introduction: Objectives \u0026amp; Expected Outcomes\nBackend Workshop (BE) ‚Äî Build API + Automated Data Import Pipeline\nPrepare \u0026amp; upload CSV data to S3 Create Lambda to automatically write CSV data to DynamoDB (S3 Trigger) Create API Gateway and integrate Lambda as Backend API Test API via Postman / API Gateway Console AI Workshop (Chatbot) ‚Äî Product Consultation Support\n(Create VPC \u0026amp; Configure Security Groups for RDS and Lambda) (Configure RDS and connect to Dbeaver) (Creating logic for lambda function) Frontend Workshop (FE) ‚Äî Display Data \u0026amp; Hosting website\nHosting website with S3 Distribute with CloudFront API Integration Set Up CI/CD for Automatic Deployment\nResource Cleanup to Avoid Unnecessary Charges\nThis workshop is designed to run within the AWS Free Tier,\nusing no EC2, no SSH, and no paid services beyond free tier limits.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/6-cleanup/",
	"title": "Clean Up Resources to Avoid AWS Charges",
	"tags": [],
	"description": "",
	"content": "Clean Up AWS Resources To avoid unexpected charges, you need to delete the AWS resources created during this workshop in the following order:\n1. Delete EventBridge Rule Go to AWS Console ‚Üí EventBridge Select Rules Select the DatabaseBackup rule Click Delete Confirm deletion 2. Delete API Gateway Go to AWS Console ‚Üí API Gateway Select FlyoraAPI Choose Actions ‚Üí Delete API Confirm deletion by entering the API name Click Delete 3. Delete Lambda Functions Go to AWS Console ‚Üí Lambda Delete the following Lambda functions: DynamoDB_API_Handler AutoImportCSVtoDynamoDB DatabaseBackupFunction For each function: Select the function ‚Üí Actions ‚Üí Delete Confirm deletion 4. Delete DynamoDB Tables Go to AWS Console ‚Üí DynamoDB Select Tables Delete all tables created from CSV files: Select a table ‚Üí Delete Confirm deletion by typing delete Repeat for all tables (Products, Orders, Customers, Reviews, etc.) 5. Delete S3 Buckets and Objects 5.1. Delete Database Bucket Go to AWS Console ‚Üí S3 Select the flyora-bucket-database bucket Delete all objects in the bucket: Click Empty bucket Confirm by typing permanently delete After the bucket is empty: Select the bucket ‚Üí Delete bucket Confirm by entering the bucket name 5.2. Delete Backup Bucket Select the flyora-bucket-backup bucket Delete all objects: Click Empty bucket Confirm deletion Delete the bucket: Click Delete bucket Confirm by entering the bucket name 6. Delete IAM User and Access Key Go to AWS Console ‚Üí IAM ‚Üí Users Select the test user Go to the Security credentials tab Delete the Access Key you created: Select the Access Key ‚Üí Actions ‚Üí Delete Return to the Users list Select the test user ‚Üí Delete user Confirm deletion 7. Delete IAM Roles 7.1. Delete LambdaAPIAccessRole Go to AWS Console ‚Üí IAM ‚Üí Roles Select LambdaAPIAccessRole Detach the attached policies: AmazonDynamoDBFullAccess CloudWatchLogsFullAccess AWSXRayDaemonWriteAccess Click Delete role Confirm deletion 7.2. Delete LambdaS3DynamoDBRole Select LambdaS3DynamoDBRole Detach the policies: AmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Click Delete role Confirm deletion 7.3. Delete LambdaDynamoDBBackupRole Select LambdaDynamoDBBackupRole Detach the policies: AmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Click Delete role Confirm deletion 8. Delete CloudWatch Logs Go to AWS Console ‚Üí CloudWatch Select Logs ‚Üí Log groups Find and delete the related log groups: /aws/lambda/DynamoDB_API_Handler /aws/lambda/AutoImportCSVtoDynamoDB /aws/lambda/DatabaseBackupFunction /aws/apigateway/FlyoraAPI Select log group ‚Üí Actions ‚Üí Delete log group(s) Confirm deletion 9. Delete X-Ray Traces (Optional) X-Ray traces automatically expire after 30 days and don\u0026rsquo;t incur storage charges, but you can manually delete them if desired.\nGo to AWS Console ‚Üí X-Ray Select Traces Traces will be automatically deleted after the default retention period 10. Delete RDS and Subnet groups Go to Subnet groups, select the created subnet group, and click Delete. Go to Databases, select the created database ‚Üí Actions ‚Üí Delete. 11. Delete BirdShopChatBot Lambda and Layer Go to Functions, select BirdShopChatBot ‚Üí Actions ‚Üí Delete. Go to Layers, select the created layer, and click Delete. 12. Delete VPC, NAT Gateway, Elastic IP, and EC2 Go to VPC, select the created NAT gateway ‚Üí Actions ‚Üí Delete NAT gateway. Select Elastic IPs ‚Üí Actions ‚Üí Release Elastic IP addresses. After deleting the NAT gateway and Elastic IP, go to Your VPCs, select the created VPC ‚Üí Actions ‚Üí Delete VPC. Go to EC2, select Instances, choose the created EC2 instance ‚Üí Instance state ‚Üí Terminate instance. 13. Delete Cloudfront Go to CloudFront, select the created distribution ‚Üí Actions ‚Üí Disable. Wait until the status changes to Disabled. Select the checkbox for the disabled distribution again. Choose Delete and confirm the deletion. The distribution cannot be recovered once deleted. Final Verification After completing the steps above, verify the following services to ensure no resources remain:\n‚úÖ EventBridge: No rules remaining ‚úÖ API Gateway: No APIs remaining ‚úÖ Lambda: No functions remaining (3 functions) ‚úÖ DynamoDB: No tables remaining ‚úÖ S3: No buckets remaining (2 buckets) ‚úÖ Cloudfront: No more distribution ‚úÖ IAM Users: No test user remaining ‚úÖ IAM Roles: No created roles remaining (3 roles) ‚úÖ CloudWatch Logs: No related log groups remaining ‚úÖ X-Ray: Traces will expire automatically ‚úÖ RDS: Successfully deleted ‚úÖ NAT gateway: No longer exists ‚úÖ Elastic IP: No longer exists ‚úÖ EC2 : Terminated Make sure you\u0026rsquo;ve deleted all resources to avoid unexpected charges. Pay special attention to S3 buckets as they can accumulate data over time.\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/2.6/",
	"title": "S3 CSV Backup",
	"tags": [],
	"description": "",
	"content": "S3 CSV Backup Create IAM Role for Lambda Go to AWS Management Console ‚Üí search for IAM. Select Roles ‚Üí Create Role. Choose Trusted entity type: AWS service. Choose Use case: Lambda, then click Next. Attach Permissions to the Role Attach the following policies:\nAmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Click Next, then name the role: LambdaDynamoDBBackupRole.\nThis role allows the Lambda function to scan all DynamoDB tables and store the backup as CSV files in an S3 bucket.\nCreate S3 Bucket Open the S3 service. In the S3 dashboard, click Create bucket. In the Create bucket screen:\nBucket name: Enter a name, for example:\nflyora-bucket-backup (If the name is already taken, add a number at the end.)\nLeave all other configuration settings as default.\nReview the configuration and click Create bucket to finish. Configure Lambda Trigger for S3 Create Lambda Function Go to Lambda ‚Üí Create function. Choose Author from scratch Name: DatabaseBackupFunction Runtime: Python 3.14 Role: select LambdaDynamoDBBackupRole created earlier Go to Configuration ‚Üí Environment variables Click Edit Add environment variable: Key: BUCKET_NAME Value: flyora-bucket-backup Click Save Code\nimport boto3 import csv import io import os from datetime import datetime from boto3.dynamodb.conditions import Key s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def scan_all(table): \u0026#34;\u0026#34;\u0026#34;Scan to√†n b·ªô b·∫£ng DynamoDB, x·ª≠ l√Ω paging.\u0026#34;\u0026#34;\u0026#34; items = [] response = table.scan() items.extend(response.get(\u0026#39;Items\u0026#39;, [])) while \u0026#39;LastEvaluatedKey\u0026#39; in response: response = table.scan(ExclusiveStartKey=response[\u0026#39;LastEvaluatedKey\u0026#39;]) items.extend(response.get(\u0026#39;Items\u0026#39;, [])) return items def lambda_handler(event, context): bucket = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] tables = [t.strip() for t in os.environ[\u0026#34;TABLE_LIST\u0026#34;].split(\u0026#34;,\u0026#34;)] timestamp = datetime.utcnow().strftime(\u0026#34;%Y-%m-%d-%H-%M-%S\u0026#34;) for table_name in tables: try: table = dynamodb.Table(table_name) data = scan_all(table) if not data: print(f\u0026#34;Table {table_name} EMPTY ‚Üí skip\u0026#34;) continue # L·∫•y danh s√°ch t·∫•t c·∫£ fields all_keys = sorted({key for item in data for key in item.keys()}) # Convert to CSV csv_buffer = io.StringIO() writer = csv.DictWriter(csv_buffer, fieldnames=all_keys) writer.writeheader() for item in data: writer.writerow({k: item.get(k, \u0026#34;\u0026#34;) for k in all_keys}) key = f\u0026#34;dynamo_backup/{table_name}/{table_name}_{timestamp}.csv\u0026#34; s3.put_object( Bucket=bucket, Key=key, Body=csv_buffer.getvalue().encode(\u0026#34;utf-8\u0026#34;) ) print(f\u0026#34;Backup xong b·∫£ng {table_name} ‚Üí {key}\u0026#34;) except Exception as e: print(f\u0026#34;L·ªói khi backup b·∫£ng {table_name}: {e}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tables\u0026#34;: tables } Click Deploy in the Lambda console. Configure Automatic Schedule Go to Lambda ‚Üí Triggers ‚Üí Add Trigger ‚Üí EventBridge (Schedule)\nRule name: DatabaseBackup\nRule description: AutoBackup in 4 days\nRule type: Schedule expression\nSchedule expression: rate(4 days)\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/6.-self-ass/",
	"title": "Self-evaluation",
	"tags": [],
	"description": "",
	"content": "Self-evaluation During my internship at AWS Vietnam from 7/9/2025 to 24/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a bootcamp environment. I participated in Backend CI/CD for Flyora project - an E-commerce platform, through which I improved my skills in [Amazon well-architectured services].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking\nEnhance communication skills in both daily interactions and professional contexts, including handling situations effectively\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/5-workshop/2-backend/2.7/",
	"title": "Integrating AWS X-Ray",
	"tags": [],
	"description": "",
	"content": "Objective Use AWS X-Ray to trace and inspect the entire processing flow of API Gateway ‚Üí Lambda ‚Üí DynamoDB.\nX-Ray helps visualize traces, latency, errors, and segments/subsegments to ensure the API behaves correctly and efficiently.\nImplementation Steps 1. Access IAM Service Go to AWS Console ‚Üí IAM Select Roles ‚Üí LambdaAPIAccessRole Choose Add permissions ‚Üí Attach policies ‚Üí AWSXRayDaemonWriteAccess 2. Access Lambda Go to AWS Console ‚Üí Lambda Select Functions ‚Üí DynamoDB_API_Handler Go to\nConfiguration ‚Üí Monitoring and operations tools ‚Üí Additional monitoring tools ‚Üí Edit Under Lambda service traces, enable Enable 3. Access API Gateway Go to AWS Console ‚Üí API Gateway Select APIs ‚Üí FlyoraAPI Go to\nStages ‚Üí Logs and tracing ‚Üí Edit Check X-Ray tracing 4. Testing Go to AWS Console ‚Üí Lambda In the Test tab, click Create new event\nEvent name: test\nPaste the JSON below into the event:\n{\r\u0026#34;resource\u0026#34;: \u0026#34;/{myProxy+}\u0026#34;,\r\u0026#34;path\u0026#34;: \u0026#34;/api/v1/bird-types\u0026#34;,\r\u0026#34;httpMethod\u0026#34;: \u0026#34;GET\u0026#34;,\r\u0026#34;headers\u0026#34;: {},\r\u0026#34;multiValueHeaders\u0026#34;: {},\r\u0026#34;queryStringParameters\u0026#34;: {},\r\u0026#34;multiValueQueryStringParameters\u0026#34;: {},\r\u0026#34;pathParameters\u0026#34;: {},\r\u0026#34;stageVariables\u0026#34;: {},\r\u0026#34;requestContext\u0026#34;: {\r\u0026#34;identity\u0026#34;: {}\r},\r\u0026#34;body\u0026#34;: null,\r\u0026#34;isBase64Encoded\u0026#34;: false\r} Save -\u0026gt; Test\nNext, go to AWS Console ‚Üí X-ray In tab Traces, a new trace ID will appear Click it to view detailed trace information\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/7.-sharing-and-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Sharing and Feedback Overall Evaluation\nWorking Environment The working environment is very friendly and open. However, I think it would be nice to have more games/social gatherings or team bonding activities to strengthen relationships.\nSupport from Mentor / Team Admin The mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\nRelevance of Work to Academic Major The tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\nLearning \u0026amp; Skill Development Opportunities During the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\nCompany Culture \u0026amp; Team Spirit The company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\nInternship Policies / Benefits The company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? Having a good member in my team\nWhat do you think the company should improve for future interns? More hands-on workshops\nIf recommending to a friend, would you suggest they intern here? Why or why not? Yes if he wants new experience, no if he wants to dive deep into real work rather than bootcamp\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? More specific tasks rather than workshops\nWould you like to continue this program in the future? Yes, I would like to continue if possible.\nAny other comments (free sharing): Love the Data Resilience workshop of Paul Haverfield\n"
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/1.-worklog/week3/",
	"title": "Week 3: Storage &amp; Databases",
	"tags": [],
	"description": "",
	"content": "Week 3: Storage \u0026amp; Databases Week 3 Objectives Implement S3 storage solutions Configure EBS volumes Set up RDS databases Work with DynamoDB NoSQL Tasks to be carried out: Day Task Start Date Completion Date Reference Material (Link) 1 S3 Bucket Creation and Configuration 21/9/2025 21/9/2025 S3 User Guide 2 S3 Storage Classes and Lifecycle 22/9/2025 22/9/2025 S3 Storage Classes 3 EBS Volume Management 23/9/2025 23/9/2025 EBS User Guide 4 RDS Database Setup 24/9/2025 24/9/2025 RDS User Guide 5 DynamoDB Table Design 25/9/2025 25/9/2025 DynamoDB Developer Guide 6 Database Migration Workshop 26/9/2025 26/9/2025 Workshop Materials 7 Storage and Database Assessment 27/9/2025 27/9/2025 Practice Scenarios Week 3 Achievements ‚úÖ Configured S3 buckets with proper security and lifecycle policies ‚úÖ Implemented EBS storage solutions with encryption and backup ‚úÖ Deployed RDS databases with Multi-AZ and read replicas ‚úÖ Designed efficient DynamoDB tables with proper indexing ‚úÖ Completed database migration scenarios successfully "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/2.-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "üê¶ Proposal: Flyora ‚Äì E-commerce Platform for Bird Lovers üìÑ Download Full Proposal PDF\n1. Executive Summary Flyora is a specialized web application designed to serve bird enthusiasts across Vietnam. It offers curated products such as bird food, toys, cages, and decorative accessories tailored to species like Ch√†o M√†o, V·∫πt, Y·∫øn Ph·ª•ng, and Ch√≠ch Ch√≤e. Built with modern web technologies and hosted on AWS, Flyora ensures scalability, performance, and secure access. The platform aims to become the go-to destination for bird care and ornamentation, combining e-commerce with personalization and community engagement.\n2. Problem Statement Current Challenges:\nNo centralized platform for bird-specific products Generic pet stores lack species-specific recommendations Poor mobile responsiveness and outdated UI in existing platforms Limited backend scalability and search capabilities Proposed Solution: Flyora delivers a responsive, category-driven shopping experience with secure user authentication, real-time product filtering, and a scalable backend. It supports both desktop and mobile users, with future plans for AI-powered recommendations and chatbot support.\n3. Solution Architecture üìÑ System Architecture Diagram üß© Frontend (Web Tier) Amazon S3: Static web hosting for frontend assets CloudFront: CDN for global content delivery Responsive design: Mobile-friendly interface üîê Authentication \u0026amp; Security IAM: Identity and access management CloudWatch \u0026amp; AWS X-Ray: Monitoring and distributed tracing üîÑ Backend Services (App Tier) Technical Improvements: Responsive, mobile-friendly UI Secure user authentication and role management (IAM) Scalable backend with Lambda/API Gateway AWS Lambda Functions: Chatbot handler Import automation API handler Amazon Bedrock: Embedding Model and LLM Model for AI-powered features üì¶ Data \u0026amp; Storage (Data Tier) Amazon RDS for PostgreSQL: Relational database DynamoDB: NoSQL database Amazon S3: Data storage üîß CI/CD \u0026amp; Development GitLab: Version control and CI/CD pipeline triggers AWS CodeBuild: Automated build process AWS CodePipeline: Continuous integration and deployment 4. Technical Implementation Phases: AWS Learning \u0026amp; Setup ‚Äì Master AWS services and architecture design Development \u0026amp; Integration ‚Äì Build frontend and connect AWS backend Testing \u0026amp; Deployment ‚Äì Complete testing and production release Month 1 - AWS Learning Focus: Week 1-2: AWS fundamentals (S3, Lambda, API Gateway, DynamoDB) Week 3: Advanced services (Bedrock, OpenSearch) Week 4: Architecture design and database modeling with MySQL Workbench Technical Requirements: AWS services proficiency for serverless architecture Frontend development with S3 static hosting DynamoDB for NoSQL data management GitHub for version control and CI/CD integration 5. Timeline \u0026amp; Milestones Phase Duration Key Milestones Month 1: AWS Learning 4 weeks ‚Ä¢ AWS fundamentals mastered‚Ä¢ Architecture designed‚Ä¢ Database schema created Month 2: Development 4 weeks ‚Ä¢ Frontend UI completed‚Ä¢ Lambda functions built‚Ä¢ API Gateway configured Month 3: Integration 4 weeks ‚Ä¢ Full system integration‚Ä¢ Testing completed‚Ä¢ Production deployment 6. Budget Estimation Item Monthly Cost Annual Cost Amazon S3 (Simple Storage Service) $0.15 $1.80 AWS Lambda (Serverless Compute) $0.00 $0.00 Amazon API Gateway (REST API Endpoints) $0.04 $0.48 DynamoDB (On-demand NoSQL Database) $0.00 $0.00 AWS X-Ray (Application Monitoring) $0.01 $0.12 Amazon CloudWatch (Monitoring \u0026amp; Logs) $0.00 $0.00 Amazon Bedrock (AI/LLM Services) $3.49 $41.88 Amazon RDS for PostgreSQL (Relational DB) $21.01 $252.12 AWS Data Transfer (Network Traffic) $0.00 $0.00 Amazon CloudFront (CDN Service) $0.10 $1.20 AWS CodePipeline (CI/CD Automation) $0.00 $0.00 AWS CodeBuild (Build Service) $2.52 $30.24 Amazon VPC (Virtual Private Cloud) $43.07 $516.84 Total Estimate $70.39 $844.68 Item Monthly Cost Annual Cost \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- Amazon S3 $0.15 $1.8 AWS Lambda $0.00 $0.00 Amazon API Gateway (REST API) $0.04 $0.48 DynamoDB (on-demand capacity) $0.00 $0.00 X-ray $0.01 $0.12 CloudWatch \u0026amp; Logs $0.00 $0.00 Amazon Bedrock (Embedding/LLM) $3.49 $41.88 Amazon RDS for PostgreSQL $21.01 $252.12 Data transfer $0.00 $0.00 CloudFront $0.10 $1.2 CodePipeline $0.00 $0.00 CodeBuild $2.52 $30.24 VPC $43.07 $516.84 Total Estimate $70.39 $844.68 Hardware costs are not applicable as Flyora is a web-only platform.\n7. Risk Assessment Risk Impact Probability Mitigation Strategy Lambda cold starts Medium Medium Provisioned concurrency for critical functions DynamoDB throttling Medium Low Auto-scaling and proper partition key design RDS downtime Medium Low Multi-AZ deployment, automated backups Cost overruns Low Low Monitor with AWS Budgets and CloudWatch alerts Bedrock API limits Medium Low Monitor usage, fallback to cached results 8. Expected Outcomes Technical Improvements: Responsive, mobile-friendly, UI/UX Secure user authentication and role management (IAM) Scalable backend with Lambda/API Gateway Real-time product filtering and chatbot support AI-powered features via Bedrock (Embedding/LLM) Robust data storage with RDS, DynamoDB, and S3 Business Value: Centralized platform for bird lovers in Vietnam Reduced reliance on generic pet stores Foundation for future AI features and community expansion Potential for mobile app and chatbot integration "
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://dophucduy.github.io/AWS_InternshipReport_FCJ/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]